{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTPtZfSEo++h/dB0qy2M5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrianda/jurnal/blob/main/Jurnal_C5_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Klasifikasi C5.0 Split data 80:20 Tanpa Menggunakan AdaBoot**"
      ],
      "metadata": {
        "id": "oYT4y9Bp4PVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from graphviz import Digraph\n",
        "\n",
        "# --- Load data ---\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")  # Ganti dengan file Anda\n",
        "# --- Pilih Kolom yang Relevan (sesuaikan jika berbeda) ---\n",
        "data = data[['class',  'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status','purpose','savings_status','personal_status']].copy()\n",
        "\n",
        "# --- Tangani missing value ---\n",
        "data.fillna(data.median(numeric_only=True), inplace=True)\n",
        "for col in data.select_dtypes(include='object'):\n",
        "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "\n",
        "# --- Split data menjadi fitur dan target ---\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# --- Encode fitur kategorikal ---\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# --- Split 80:20 ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# --- Entropy Function ---\n",
        "def entropy(labels):\n",
        "    counts = labels.value_counts()\n",
        "    total = len(labels)\n",
        "    ent = 0\n",
        "    for count in counts:\n",
        "        p = count / total\n",
        "        ent -= p * math.log2(p)\n",
        "    return ent\n",
        "\n",
        "# --- Gain Ratio for Numeric ---\n",
        "def gain_ratio_numeric(data, attr, target, threshold):\n",
        "    total_entropy = entropy(data[target])\n",
        "    left = data[data[attr] <= threshold]\n",
        "    right = data[data[attr] > threshold]\n",
        "\n",
        "    left_entropy = entropy(left[target]) if len(left) > 0 else 0\n",
        "    right_entropy = entropy(right[target]) if len(right) > 0 else 0\n",
        "\n",
        "    weighted_entropy = (len(left) / len(data)) * left_entropy + (len(right) / len(data)) * right_entropy\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "\n",
        "    split_info = 0\n",
        "    for subset in [left, right]:\n",
        "        p = len(subset) / len(data)\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    if split_info == 0:\n",
        "        return 0, None\n",
        "\n",
        "    return info_gain / split_info, threshold\n",
        "\n",
        "# --- Gain Ratio for Categorical ---\n",
        "def gain_ratio_categorical(data, attr, target):\n",
        "    total_entropy = entropy(data[target])\n",
        "    subsets = data.groupby(attr)\n",
        "\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for key, subset in subsets:\n",
        "        p = len(subset) / len(data)\n",
        "        weighted_entropy += p * entropy(subset[target])\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "# --- Get thresholds for numeric attributes ---\n",
        "def get_thresholds(column):\n",
        "    unique_vals = sorted(column.unique())\n",
        "    thresholds = []\n",
        "    for i in range(len(unique_vals) - 1):\n",
        "        thresholds.append((unique_vals[i] + unique_vals[i + 1]) / 2)\n",
        "    return thresholds\n",
        "\n",
        "# --- Find best attribute and threshold ---\n",
        "def best_split(data, target, attributes):\n",
        "    best_gain_ratio = -1\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in ['int64', 'float64']:\n",
        "            thresholds = get_thresholds(data[attr])\n",
        "            for t in thresholds:\n",
        "                gr, thr = gain_ratio_numeric(data, attr, target, t)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = thr\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio_categorical(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "\n",
        "    return best_attr, best_threshold, best_gain_ratio, is_numeric\n",
        "\n",
        "# --- Build decision tree ---\n",
        "def build_tree(data, target, attributes, depth=0, max_depth=4):\n",
        "    labels = data[target]\n",
        "    if len(labels.unique()) == 1:\n",
        "        return labels.iloc[0]\n",
        "    if depth == max_depth or len(attributes) == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    attr, threshold, gr, is_num = best_split(data, target, attributes)\n",
        "    if attr is None or gr == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    tree = {}\n",
        "    if is_num:\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"] = {}\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['Yes'] = build_tree(left, target, attributes, depth + 1, max_depth)\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['No'] = build_tree(right, target, attributes, depth + 1, max_depth)\n",
        "    else:\n",
        "        tree[f\"{attr}\"] = {}\n",
        "        for val in data[attr].unique():\n",
        "            subset = data[data[attr] == val]\n",
        "            tree[f\"{attr}\"][val] = build_tree(subset, target, [a for a in attributes if a != attr], depth + 1, max_depth)\n",
        "    return tree\n",
        "\n",
        "# --- Predict with tree ---\n",
        "def predict(tree, sample):\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree\n",
        "    root = next(iter(tree))\n",
        "    if \"<=\" in root:\n",
        "        attr, thresh = root.split(\" <= \")\n",
        "        thresh = float(thresh)\n",
        "        if sample[attr] <= thresh:\n",
        "            return predict(tree[root]['Yes'], sample)\n",
        "        else:\n",
        "            return predict(tree[root]['No'], sample)\n",
        "    else:\n",
        "        attr = root\n",
        "        val = sample[attr]\n",
        "        if val in tree[attr]:\n",
        "            return predict(tree[attr][val], sample)\n",
        "        else:\n",
        "            return list(tree[attr].values())[0]\n",
        "\n",
        "# --- Visualize tree ---\n",
        "def visualize_tree(tree, dot=None, parent=None, edge_label=None):\n",
        "    if dot is None:\n",
        "        dot = Digraph()\n",
        "        dot.attr('node', shape='ellipse', fontname='Arial')\n",
        "\n",
        "    if not isinstance(tree, dict):\n",
        "        node_id = str(id(tree)) + str(np.random.randint(1000))\n",
        "        dot.node(node_id, label=str(tree), shape='box', style='filled', color='lightgrey')\n",
        "        if parent is not None:\n",
        "            dot.edge(parent, node_id, label=edge_label)\n",
        "        return dot\n",
        "\n",
        "    root = next(iter(tree))\n",
        "    node_id = str(id(root)) + str(np.random.randint(1000))\n",
        "    dot.node(node_id, label=root)\n",
        "    if parent is not None:\n",
        "        dot.edge(parent, node_id, label=edge_label)\n",
        "\n",
        "    children = tree[root]\n",
        "    for branch_label, subtree in children.items():\n",
        "        visualize_tree(subtree, dot, node_id, str(branch_label))\n",
        "    return dot\n",
        "\n",
        "# --- Main ---\n",
        "attributes = list(X.columns)\n",
        "target = 'class'\n",
        "\n",
        "tree = build_tree(train_data, target, attributes, max_depth=4)\n",
        "\n",
        "# --- Prediksi dan Evaluasi ---\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for _, row in test_data.iterrows():\n",
        "    sample = row.drop('class')\n",
        "    pred = predict(tree, sample)\n",
        "    y_pred.append(pred)\n",
        "    y_true.append(row['class'])\n",
        "\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(f\"Akurasi: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "# --- Visualisasi pohon keputusan ---\n",
        "#dot = visualize_tree(tree)\n",
        "#dot.render('credit_tree', format='png', cleanup=True)\n",
        "#print(\"Pohon keputusan tersimpan di file credit_tree.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQnZfQVT4OjY",
        "outputId": "47c06e29-39f4-420f-b2d3-83ce957afb8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f329a504cb51>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[col].fillna(data[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         bad       0.00      0.00      0.00        59\n",
            "        good       0.70      1.00      0.83       141\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.35      0.50      0.41       200\n",
            "weighted avg       0.50      0.70      0.58       200\n",
            "\n",
            "Akurasi: 0.7050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Klasifikasi C5.0 Split data 70:30 Tanpa Menggunakan AdaBoot**"
      ],
      "metadata": {
        "id": "d42ICDfN7FLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from graphviz import Digraph\n",
        "\n",
        "# --- Load data ---\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")  # Ganti dengan file Anda\n",
        "# --- Pilih Kolom yang Relevan (sesuaikan jika berbeda) ---\n",
        "data = data[['class',  'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status','purpose','savings_status','personal_status']].copy()\n",
        "\n",
        "# --- Tangani missing value ---\n",
        "data.fillna(data.median(numeric_only=True), inplace=True)\n",
        "for col in data.select_dtypes(include='object'):\n",
        "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "\n",
        "# --- Split data menjadi fitur dan target ---\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# --- Encode fitur kategorikal ---\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# --- Split 80:20 ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# --- Entropy Function ---\n",
        "def entropy(labels):\n",
        "    counts = labels.value_counts()\n",
        "    total = len(labels)\n",
        "    ent = 0\n",
        "    for count in counts:\n",
        "        p = count / total\n",
        "        ent -= p * math.log2(p)\n",
        "    return ent\n",
        "\n",
        "# --- Gain Ratio for Numeric ---\n",
        "def gain_ratio_numeric(data, attr, target, threshold):\n",
        "    total_entropy = entropy(data[target])\n",
        "    left = data[data[attr] <= threshold]\n",
        "    right = data[data[attr] > threshold]\n",
        "\n",
        "    left_entropy = entropy(left[target]) if len(left) > 0 else 0\n",
        "    right_entropy = entropy(right[target]) if len(right) > 0 else 0\n",
        "\n",
        "    weighted_entropy = (len(left) / len(data)) * left_entropy + (len(right) / len(data)) * right_entropy\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "\n",
        "    split_info = 0\n",
        "    for subset in [left, right]:\n",
        "        p = len(subset) / len(data)\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    if split_info == 0:\n",
        "        return 0, None\n",
        "\n",
        "    return info_gain / split_info, threshold\n",
        "\n",
        "# --- Gain Ratio for Categorical ---\n",
        "def gain_ratio_categorical(data, attr, target):\n",
        "    total_entropy = entropy(data[target])\n",
        "    subsets = data.groupby(attr)\n",
        "\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for key, subset in subsets:\n",
        "        p = len(subset) / len(data)\n",
        "        weighted_entropy += p * entropy(subset[target])\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "# --- Get thresholds for numeric attributes ---\n",
        "def get_thresholds(column):\n",
        "    unique_vals = sorted(column.unique())\n",
        "    thresholds = []\n",
        "    for i in range(len(unique_vals) - 1):\n",
        "        thresholds.append((unique_vals[i] + unique_vals[i + 1]) / 2)\n",
        "    return thresholds\n",
        "\n",
        "# --- Find best attribute and threshold ---\n",
        "def best_split(data, target, attributes):\n",
        "    best_gain_ratio = -1\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in ['int64', 'float64']:\n",
        "            thresholds = get_thresholds(data[attr])\n",
        "            for t in thresholds:\n",
        "                gr, thr = gain_ratio_numeric(data, attr, target, t)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = thr\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio_categorical(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "\n",
        "    return best_attr, best_threshold, best_gain_ratio, is_numeric\n",
        "\n",
        "# --- Build decision tree ---\n",
        "def build_tree(data, target, attributes, depth=0, max_depth=4):\n",
        "    labels = data[target]\n",
        "    if len(labels.unique()) == 1:\n",
        "        return labels.iloc[0]\n",
        "    if depth == max_depth or len(attributes) == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    attr, threshold, gr, is_num = best_split(data, target, attributes)\n",
        "    if attr is None or gr == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    tree = {}\n",
        "    if is_num:\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"] = {}\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['Yes'] = build_tree(left, target, attributes, depth + 1, max_depth)\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['No'] = build_tree(right, target, attributes, depth + 1, max_depth)\n",
        "    else:\n",
        "        tree[f\"{attr}\"] = {}\n",
        "        for val in data[attr].unique():\n",
        "            subset = data[data[attr] == val]\n",
        "            tree[f\"{attr}\"][val] = build_tree(subset, target, [a for a in attributes if a != attr], depth + 1, max_depth)\n",
        "    return tree\n",
        "\n",
        "# --- Predict with tree ---\n",
        "def predict(tree, sample):\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree\n",
        "    root = next(iter(tree))\n",
        "    if \"<=\" in root:\n",
        "        attr, thresh = root.split(\" <= \")\n",
        "        thresh = float(thresh)\n",
        "        if sample[attr] <= thresh:\n",
        "            return predict(tree[root]['Yes'], sample)\n",
        "        else:\n",
        "            return predict(tree[root]['No'], sample)\n",
        "    else:\n",
        "        attr = root\n",
        "        val = sample[attr]\n",
        "        if val in tree[attr]:\n",
        "            return predict(tree[attr][val], sample)\n",
        "        else:\n",
        "            return list(tree[attr].values())[0]\n",
        "\n",
        "# --- Visualize tree ---\n",
        "def visualize_tree(tree, dot=None, parent=None, edge_label=None):\n",
        "    if dot is None:\n",
        "        dot = Digraph()\n",
        "        dot.attr('node', shape='ellipse', fontname='Arial')\n",
        "\n",
        "    if not isinstance(tree, dict):\n",
        "        node_id = str(id(tree)) + str(np.random.randint(1000))\n",
        "        dot.node(node_id, label=str(tree), shape='box', style='filled', color='lightgrey')\n",
        "        if parent is not None:\n",
        "            dot.edge(parent, node_id, label=edge_label)\n",
        "        return dot\n",
        "\n",
        "    root = next(iter(tree))\n",
        "    node_id = str(id(root)) + str(np.random.randint(1000))\n",
        "    dot.node(node_id, label=root)\n",
        "    if parent is not None:\n",
        "        dot.edge(parent, node_id, label=edge_label)\n",
        "\n",
        "    children = tree[root]\n",
        "    for branch_label, subtree in children.items():\n",
        "        visualize_tree(subtree, dot, node_id, str(branch_label))\n",
        "    return dot\n",
        "\n",
        "# --- Main ---\n",
        "attributes = list(X.columns)\n",
        "target = 'class'\n",
        "\n",
        "tree = build_tree(train_data, target, attributes, max_depth=4)\n",
        "\n",
        "# --- Prediksi dan Evaluasi ---\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for _, row in test_data.iterrows():\n",
        "    sample = row.drop('class')\n",
        "    pred = predict(tree, sample)\n",
        "    y_pred.append(pred)\n",
        "    y_true.append(row['class'])\n",
        "\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(f\"Akurasi: {accuracy_score(y_true, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhrxh1C47Fl0",
        "outputId": "f249e008-3524-45ff-c817-029193120adc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ebca974aa281>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[col].fillna(data[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         bad       0.00      0.00      0.00        91\n",
            "        good       0.70      1.00      0.82       209\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.35      0.50      0.41       300\n",
            "weighted avg       0.49      0.70      0.57       300\n",
            "\n",
            "Akurasi: 0.6967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Kedalaman Pohon Split Data 80:20 Tanpa Adaboots**"
      ],
      "metadata": {
        "id": "Va_7ICYv2YYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Load dataset ---\n",
        "# Gantilah ini dengan pemanggilan dataset asli Anda\n",
        "# Contoh:\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "# --- Pilih Kolom yang Relevan (sesuaikan jika berbeda) ---\n",
        "data = data[['class',  'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status','purpose','savings_status','personal_status']].copy()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Ubah kolom numerik bertipe string ke float\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass  # Tetap string jika tidak bisa dikonversi\n",
        "\n",
        "# Encode fitur kategorikal\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Gabungkan kembali dengan target untuk imputasi nilai hilang\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "\n",
        "# Isi nilai kosong numerik dengan median\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Isi nilai kosong kategorikal dengan modus\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "# Split dataset 80:20\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# --- Entropy Function ---\n",
        "def entropy(labels):\n",
        "    counts = labels.value_counts()\n",
        "    total = len(labels)\n",
        "    ent = 0\n",
        "    for count in counts:\n",
        "        p = count / total\n",
        "        ent -= p * math.log2(p)\n",
        "    return ent\n",
        "\n",
        "# --- Gain Ratio for Numeric ---\n",
        "def gain_ratio_numeric(data, attr, target, threshold):\n",
        "    total_entropy = entropy(data[target])\n",
        "    left = data[data[attr] <= threshold]\n",
        "    right = data[data[attr] > threshold]\n",
        "\n",
        "    left_entropy = entropy(left[target]) if len(left) > 0 else 0\n",
        "    right_entropy = entropy(right[target]) if len(right) > 0 else 0\n",
        "\n",
        "    weighted_entropy = (len(left) / len(data)) * left_entropy + (len(right) / len(data)) * right_entropy\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "\n",
        "    split_info = 0\n",
        "    for subset in [left, right]:\n",
        "        p = len(subset) / len(data)\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    if split_info == 0:\n",
        "        return 0, None\n",
        "\n",
        "    return info_gain / split_info, threshold\n",
        "\n",
        "# --- Gain Ratio for Categorical ---\n",
        "def gain_ratio_categorical(data, attr, target):\n",
        "    total_entropy = entropy(data[target])\n",
        "    subsets = data.groupby(attr)\n",
        "\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for key, subset in subsets:\n",
        "        p = len(subset) / len(data)\n",
        "        weighted_entropy += p * entropy(subset[target])\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "# --- Get thresholds for numeric attributes ---\n",
        "def get_thresholds(column_data):\n",
        "    unique_vals = sorted(column_data.unique())\n",
        "    thresholds = []\n",
        "    for i in range(len(unique_vals) - 1):\n",
        "        thresholds.append((unique_vals[i] + unique_vals[i + 1]) / 2)\n",
        "    return thresholds\n",
        "\n",
        "# --- Find best attribute and threshold ---\n",
        "def best_split(data, target, attributes):\n",
        "    best_gain_ratio = -1\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in ['int64', 'float64']:\n",
        "            thresholds = get_thresholds(data[attr])\n",
        "            for t in thresholds:\n",
        "                gr, thr = gain_ratio_numeric(data, attr, target, t)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = thr\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio_categorical(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "\n",
        "    return best_attr, best_threshold, best_gain_ratio, is_numeric\n",
        "\n",
        "# --- Build decision tree ---\n",
        "def build_tree(data, target, attributes, depth=0, max_depth=4):\n",
        "    labels = data[target]\n",
        "    if len(labels.unique()) == 1:\n",
        "        return labels.iloc[0]\n",
        "    if depth == max_depth or len(attributes) == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    attr, threshold, gr, is_num = best_split(data, target, attributes)\n",
        "    if attr is None or gr == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    tree = {}\n",
        "    if is_num:\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"] = {}\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['Yes'] = build_tree(left, target, attributes, depth+1, max_depth)\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['No'] = build_tree(right, target, attributes, depth+1, max_depth)\n",
        "    else:\n",
        "        tree[f\"{attr}\"] = {}\n",
        "        for val in data[attr].unique():\n",
        "            subset = data[data[attr] == val]\n",
        "            tree[f\"{attr}\"][val] = build_tree(subset, target, [a for a in attributes if a != attr], depth+1, max_depth)\n",
        "    return tree\n",
        "\n",
        "# --- Predict with tree ---\n",
        "def predict(tree, sample):\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree\n",
        "    root = next(iter(tree))\n",
        "    if \"<=\" in root:\n",
        "        attr, thresh = root.split(\" <= \")\n",
        "        thresh = float(thresh)\n",
        "        if sample[attr] <= thresh:\n",
        "            return predict(tree[root]['Yes'], sample)\n",
        "        else:\n",
        "            return predict(tree[root]['No'], sample)\n",
        "    else:\n",
        "        attr = root\n",
        "        val = sample[attr]\n",
        "        if val in tree[attr]:\n",
        "            return predict(tree[attr][val], sample)\n",
        "        else:\n",
        "            return list(tree[attr].values())[0]\n",
        "\n",
        "# --- Loop over depths 1 to 20 ---\n",
        "print(\"max_depth | accuracy | time (seconds)\")\n",
        "for depth in range(1, 16):\n",
        "    start_time = time.time()\n",
        "    attributes = list(train_data.columns)\n",
        "    attributes.remove('class')\n",
        "\n",
        "    tree = build_tree(train_data.copy(), 'class', attributes, max_depth=depth)\n",
        "\n",
        "    y_pred = []\n",
        "    for _, row in test_data.iterrows():\n",
        "        y_pred.append(predict(tree, row))\n",
        "\n",
        "    acc = accuracy_score(test_data['class'], y_pred)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"{depth:9d} | {acc:.4f}   | {elapsed:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-d7yWf33jnN",
        "outputId": "bb91412b-ca4f-4e8a-9bb1-9d0d5ddef491"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-5bdca1a588b5>:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth | accuracy | time (seconds)\n",
            "        1 | 0.7050   | 2.6369\n",
            "        2 | 0.7050   | 2.5116\n",
            "        3 | 0.7050   | 3.9678\n",
            "        4 | 0.7050   | 6.2746\n",
            "        5 | 0.7050   | 7.0622\n",
            "        6 | 0.7050   | 9.3965\n",
            "        7 | 0.6900   | 10.7387\n",
            "        8 | 0.7000   | 17.6218\n",
            "        9 | 0.7000   | 15.9473\n",
            "       10 | 0.7000   | 15.5145\n",
            "       11 | 0.7000   | 24.8951\n",
            "       12 | 0.7000   | 23.3475\n",
            "       13 | 0.7000   | 22.5093\n",
            "       14 | 0.7000   | 21.1901\n",
            "       15 | 0.7000   | 23.6150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Kedalaman Pohon Split Data 70:30 Tanpa Adaboots**"
      ],
      "metadata": {
        "id": "WJxW8Zas7h9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Load dataset ---\n",
        "# Gantilah ini dengan pemanggilan dataset asli Anda\n",
        "# Contoh:\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "# --- Pilih Kolom yang Relevan (sesuaikan jika berbeda) ---\n",
        "data = data[['class',  'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status','purpose','savings_status','personal_status']].copy()\n",
        "\n",
        "# --- Preprocessing ---\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Ubah kolom numerik bertipe string ke float\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass  # Tetap string jika tidak bisa dikonversi\n",
        "\n",
        "# Encode fitur kategorikal\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Gabungkan kembali dengan target untuk imputasi nilai hilang\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "\n",
        "# Isi nilai kosong numerik dengan median\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Isi nilai kosong kategorikal dengan modus\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "# Split dataset 80:20\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# --- Entropy Function ---\n",
        "def entropy(labels):\n",
        "    counts = labels.value_counts()\n",
        "    total = len(labels)\n",
        "    ent = 0\n",
        "    for count in counts:\n",
        "        p = count / total\n",
        "        ent -= p * math.log2(p)\n",
        "    return ent\n",
        "\n",
        "# --- Gain Ratio for Numeric ---\n",
        "def gain_ratio_numeric(data, attr, target, threshold):\n",
        "    total_entropy = entropy(data[target])\n",
        "    left = data[data[attr] <= threshold]\n",
        "    right = data[data[attr] > threshold]\n",
        "\n",
        "    left_entropy = entropy(left[target]) if len(left) > 0 else 0\n",
        "    right_entropy = entropy(right[target]) if len(right) > 0 else 0\n",
        "\n",
        "    weighted_entropy = (len(left) / len(data)) * left_entropy + (len(right) / len(data)) * right_entropy\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "\n",
        "    split_info = 0\n",
        "    for subset in [left, right]:\n",
        "        p = len(subset) / len(data)\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    if split_info == 0:\n",
        "        return 0, None\n",
        "\n",
        "    return info_gain / split_info, threshold\n",
        "\n",
        "# --- Gain Ratio for Categorical ---\n",
        "def gain_ratio_categorical(data, attr, target):\n",
        "    total_entropy = entropy(data[target])\n",
        "    subsets = data.groupby(attr)\n",
        "\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for key, subset in subsets:\n",
        "        p = len(subset) / len(data)\n",
        "        weighted_entropy += p * entropy(subset[target])\n",
        "        if p > 0:\n",
        "            split_info -= p * math.log2(p)\n",
        "\n",
        "    info_gain = total_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "# --- Get thresholds for numeric attributes ---\n",
        "def get_thresholds(column_data):\n",
        "    unique_vals = sorted(column_data.unique())\n",
        "    thresholds = []\n",
        "    for i in range(len(unique_vals) - 1):\n",
        "        thresholds.append((unique_vals[i] + unique_vals[i + 1]) / 2)\n",
        "    return thresholds\n",
        "\n",
        "# --- Find best attribute and threshold ---\n",
        "def best_split(data, target, attributes):\n",
        "    best_gain_ratio = -1\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in ['int64', 'float64']:\n",
        "            thresholds = get_thresholds(data[attr])\n",
        "            for t in thresholds:\n",
        "                gr, thr = gain_ratio_numeric(data, attr, target, t)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = thr\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio_categorical(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "\n",
        "    return best_attr, best_threshold, best_gain_ratio, is_numeric\n",
        "\n",
        "# --- Build decision tree ---\n",
        "def build_tree(data, target, attributes, depth=0, max_depth=4):\n",
        "    labels = data[target]\n",
        "    if len(labels.unique()) == 1:\n",
        "        return labels.iloc[0]\n",
        "    if depth == max_depth or len(attributes) == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    attr, threshold, gr, is_num = best_split(data, target, attributes)\n",
        "    if attr is None or gr == 0:\n",
        "        return labels.mode()[0]\n",
        "\n",
        "    tree = {}\n",
        "    if is_num:\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"] = {}\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['Yes'] = build_tree(left, target, attributes, depth+1, max_depth)\n",
        "        tree[f\"{attr} <= {threshold:.2f}\"]['No'] = build_tree(right, target, attributes, depth+1, max_depth)\n",
        "    else:\n",
        "        tree[f\"{attr}\"] = {}\n",
        "        for val in data[attr].unique():\n",
        "            subset = data[data[attr] == val]\n",
        "            tree[f\"{attr}\"][val] = build_tree(subset, target, [a for a in attributes if a != attr], depth+1, max_depth)\n",
        "    return tree\n",
        "\n",
        "# --- Predict with tree ---\n",
        "def predict(tree, sample):\n",
        "    if not isinstance(tree, dict):\n",
        "        return tree\n",
        "    root = next(iter(tree))\n",
        "    if \"<=\" in root:\n",
        "        attr, thresh = root.split(\" <= \")\n",
        "        thresh = float(thresh)\n",
        "        if sample[attr] <= thresh:\n",
        "            return predict(tree[root]['Yes'], sample)\n",
        "        else:\n",
        "            return predict(tree[root]['No'], sample)\n",
        "    else:\n",
        "        attr = root\n",
        "        val = sample[attr]\n",
        "        if val in tree[attr]:\n",
        "            return predict(tree[attr][val], sample)\n",
        "        else:\n",
        "            return list(tree[attr].values())[0]\n",
        "\n",
        "# --- Loop over depths 1 to 20 ---\n",
        "print(\"max_depth | accuracy | time (seconds)\")\n",
        "for depth in range(1, 16):\n",
        "    start_time = time.time()\n",
        "    attributes = list(train_data.columns)\n",
        "    attributes.remove('class')\n",
        "\n",
        "    tree = build_tree(train_data.copy(), 'class', attributes, max_depth=depth)\n",
        "\n",
        "    y_pred = []\n",
        "    for _, row in test_data.iterrows():\n",
        "        y_pred.append(predict(tree, row))\n",
        "\n",
        "    acc = accuracy_score(test_data['class'], y_pred)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"{depth:9d} | {acc:.4f}   | {elapsed:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoqvcOn57iHW",
        "outputId": "a3a3d6ba-32d8-41af-838f-0862e9ac0dd4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cb6c8069df62>:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth | accuracy | time (seconds)\n",
            "        1 | 0.6967   | 2.9022\n",
            "        2 | 0.6967   | 4.9136\n",
            "        3 | 0.6967   | 3.4993\n",
            "        4 | 0.6967   | 4.9144\n",
            "        5 | 0.6933   | 6.8248\n",
            "        6 | 0.6800   | 8.0470\n",
            "        7 | 0.6833   | 9.2358\n",
            "        8 | 0.6833   | 10.6600\n",
            "        9 | 0.6833   | 11.6949\n",
            "       10 | 0.6733   | 12.7369\n",
            "       11 | 0.6733   | 13.7568\n",
            "       12 | 0.6733   | 14.5447\n",
            "       13 | 0.6733   | 16.3008\n",
            "       14 | 0.6667   | 19.6154\n",
            "       15 | 0.6667   | 18.5216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pandas numpy scikit-learn graphviz"
      ],
      "metadata": {
        "id": "YKA0nXJi8dkG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Klasifikasi C5.0 Split data 80:20 Menggunakan AdaBoot**"
      ],
      "metadata": {
        "id": "uUwMiDX08mlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ================================\n",
        "# --- Load dan Preprocessing ---\n",
        "# ================================\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "\n",
        "# Pilih kolom relevan\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Ubah kolom numerik bertipe string ke float\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass  # Biarkan jika memang tetap string\n",
        "\n",
        "# Encode fitur kategorikal\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Gabungkan kembali fitur dan target\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "\n",
        "# Isi nilai kosong numerik\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Isi nilai kosong kategorikal\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "# Encode target (bad/good → 0/1)\n",
        "label_encoder_target = LabelEncoder()\n",
        "data_cleaned['class'] = label_encoder_target.fit_transform(data_cleaned['class'])\n",
        "\n",
        "# Split 80:20\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# =========================================\n",
        "# === Fungsi Algoritma Gain Ratio + AdaBoost ===\n",
        "# =========================================\n",
        "\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    thresholds = (values[:-1] + values[1:]) / 2\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in thresholds:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data)) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data)))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        if len(counts) == 0:\n",
        "            return {'type': 'leaf', 'class': 0}\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        left_data = data[data[attr] <= threshold]\n",
        "        right_data = data[data[attr] > threshold]\n",
        "        node['left'] = build_tree(left_data, target, attributes)\n",
        "        node['right'] = build_tree(right_data, target, attributes)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            subset = data[data[attr] == val]\n",
        "            node['branches'][val] = build_tree(subset, target, attributes)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "def convert_label(y):\n",
        "    return np.where(y == 1, 1, -1)\n",
        "\n",
        "def ada_boost_train(data, target, attributes, n_estimators=10):\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n\n",
        "    trees = []\n",
        "    alphas = []\n",
        "    y_true = convert_label(data[target].values)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_indices = np.random.choice(n, size=n, replace=True, p=weights)\n",
        "        sample_data = data.iloc[sample_indices].reset_index(drop=True)\n",
        "        tree = build_tree(sample_data, target, attributes)\n",
        "        trees.append(tree)\n",
        "\n",
        "        preds = np.array([1 if predict(tree, data.iloc[i]) == 1 else -1 for i in range(n)])\n",
        "        miss = (preds != y_true).astype(int)\n",
        "        error = np.sum(weights * miss)\n",
        "\n",
        "        if error == 0:\n",
        "            alpha = 1\n",
        "            alphas.append(alpha)\n",
        "            break\n",
        "        if error >= 0.5:\n",
        "            break\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        alphas.append(alpha)\n",
        "        weights *= np.exp(-alpha * y_true * preds)\n",
        "        weights /= np.sum(weights)\n",
        "    return trees, alphas\n",
        "\n",
        "def ada_boost_predict(trees, alphas, sample):\n",
        "    total = 0\n",
        "    for tree, alpha in zip(trees, alphas):\n",
        "        pred = predict(tree, sample)\n",
        "        pred_mod = 1 if pred == 1 else -1\n",
        "        total += alpha * pred_mod\n",
        "    return 1 if total >= 0 else 0\n",
        "\n",
        "# =====================================\n",
        "# === Pelatihan & Evaluasi Model ===\n",
        "# =====================================\n",
        "start = time.time()\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "trees, alphas = ada_boost_train(train_data, 'class', attributes, n_estimators=10)\n",
        "end = time.time()\n",
        "\n",
        "y_pred = [ada_boost_predict(trees, alphas, row) for _, row in test_data.iterrows()]\n",
        "y_true = test_data['class'].values\n",
        "\n",
        "# Evaluasi\n",
        "print(f\"Akurasi: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(f\"Waktu eksekusi: {end - start:.2f} detik\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdauUhez_pf3",
        "outputId": "aa1ab942-e315-4775-94a1-2f4f57db146a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-4811932b1a54>:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.7350\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.41      0.48        59\n",
            "           1       0.78      0.87      0.82       141\n",
            "\n",
            "    accuracy                           0.73       200\n",
            "   macro avg       0.67      0.64      0.65       200\n",
            "weighted avg       0.72      0.73      0.72       200\n",
            "\n",
            "Waktu eksekusi: 240.35 detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Waktu di Perbaiki AdaBoot tidak banyak looping**"
      ],
      "metadata": {
        "id": "_hzOF1G_CC-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# =============================\n",
        "# --- Load dan Preprocessing ---\n",
        "# =============================\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "\n",
        "# Pilih kolom relevan\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Ubah kolom numerik bertipe string ke float\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Encode fitur kategorikal\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Gabungkan kembali\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "\n",
        "# Isi nilai kosong numerik\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Isi nilai kosong kategorikal\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "# Encode target\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "# Split 80:20\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# ===========================================\n",
        "# === Fungsi Algoritma Gain Ratio + AdaBoost ===\n",
        "# ===========================================\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "\n",
        "def build_tree(data, target, attributes):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes)\n",
        "    return node\n",
        "\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "def convert_label(y):\n",
        "    return np.where(y == 1, 1, -1)\n",
        "\n",
        "\n",
        "def ada_boost_train(data, target, attributes, n_estimators=10):\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n\n",
        "    trees = []\n",
        "    alphas = []\n",
        "    y_true = convert_label(data[target].values)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_indices = np.random.choice(n, size=n, replace=True, p=weights)\n",
        "        sample_data = data.iloc[sample_indices].reset_index(drop=True)\n",
        "        tree = build_tree(sample_data, target, attributes)\n",
        "        trees.append(tree)\n",
        "\n",
        "        preds = np.array([1 if predict(tree, row) == 1 else -1 for _, row in data.iterrows()])\n",
        "        miss = (preds != y_true).astype(int)\n",
        "        error = np.sum(weights * miss)\n",
        "\n",
        "        if error == 0:\n",
        "            alphas.append(1)\n",
        "            break\n",
        "        if error >= 0.5:\n",
        "            break\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        alphas.append(alpha)\n",
        "        weights *= np.exp(-alpha * y_true * preds)\n",
        "        weights /= np.sum(weights)\n",
        "    return trees, alphas\n",
        "\n",
        "\n",
        "def ada_boost_predict(trees, alphas, sample):\n",
        "    total = 0\n",
        "    for tree, alpha in zip(trees, alphas):\n",
        "        pred = predict(tree, sample)\n",
        "        pred_mod = 1 if pred == 1 else -1\n",
        "        total += alpha * pred_mod\n",
        "    return 1 if total >= 0 else 0\n",
        "\n",
        "# ==========================\n",
        "# === Pelatihan & Evaluasi ===\n",
        "# ==========================\n",
        "start = time.time()\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "trees, alphas = ada_boost_train(train_data, 'class', attributes, n_estimators=10)\n",
        "end = time.time()\n",
        "\n",
        "y_pred = [ada_boost_predict(trees, alphas, row) for _, row in test_data.iterrows()]\n",
        "y_true = test_data['class'].values\n",
        "#accuracy = accuracy_score(y_true, y_pred)\n",
        "#report = classification_report(y_true, y_pred)\n",
        "#execution_time = end - start\n",
        "\n",
        "#@accuracy, report, execution_time\n",
        "# Evaluasi\n",
        "print(f\"Akurasi: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(f\"Waktu eksekusi: {end - start:.2f} detik\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kluhFDdeBHuI",
        "outputId": "88aaf62b-6a0e-49ad-ec23-d3b2307aadb4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f152143ac9f8>:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.7400\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.59      0.57        59\n",
            "           1       0.82      0.80      0.81       141\n",
            "\n",
            "    accuracy                           0.74       200\n",
            "   macro avg       0.69      0.70      0.69       200\n",
            "weighted avg       0.75      0.74      0.74       200\n",
            "\n",
            "Waktu eksekusi: 39.01 detik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Menggunakan Kedalaman Pohon**"
      ],
      "metadata": {
        "id": "jN6lt-nACpU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dan Preprocessing\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Ubah kolom numerik bertipe string ke float\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Encode fitur kategorikal\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "# Fungsi Gain Ratio dan Tree (dengan max_depth)\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes, max_depth, current_depth=0):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0 or current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "# Evaluasi untuk max_depth 1-15\n",
        "results = []\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "for depth in range(1, 16):\n",
        "    start_time = time.time()\n",
        "    tree = build_tree(train_data, 'class', attributes, max_depth=depth)\n",
        "    predictions = [predict(tree, row) for _, row in test_data.iterrows()]\n",
        "    acc = accuracy_score(test_data['class'], predictions)\n",
        "    exec_time = time.time() - start_time\n",
        "    results.append((depth, acc, exec_time))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['max_depth', 'accuracy', 'execution_time'])\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "SRvucnNXCpea",
        "outputId": "16c61061-ccef-4ea9-c124-733ea15d232d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    max_depth  accuracy  execution_time\n",
              "0           1     0.705        0.041679\n",
              "1           2     0.730        0.114796\n",
              "2           3     0.735        0.236587\n",
              "3           4     0.735        0.430345\n",
              "4           5     0.735        1.015578\n",
              "5           6     0.745        0.957056\n",
              "6           7     0.730        1.258426\n",
              "7           8     0.730        1.598124\n",
              "8           9     0.740        2.153770\n",
              "9          10     0.730        2.820723\n",
              "10         11     0.740        2.534235\n",
              "11         12     0.710        2.890296\n",
              "12         13     0.725        3.235183\n",
              "13         14     0.705        4.218148\n",
              "14         15     0.730        3.686193"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d558e189-341c-4262-b008-94b4cf33ad47\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>max_depth</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>execution_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.041679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.114796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.236587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.430345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.735</td>\n",
              "      <td>1.015578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.957056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0.730</td>\n",
              "      <td>1.258426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.730</td>\n",
              "      <td>1.598124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>0.740</td>\n",
              "      <td>2.153770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>0.730</td>\n",
              "      <td>2.820723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.740</td>\n",
              "      <td>2.534235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>0.710</td>\n",
              "      <td>2.890296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>0.725</td>\n",
              "      <td>3.235183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>0.705</td>\n",
              "      <td>4.218148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>0.730</td>\n",
              "      <td>3.686193</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d558e189-341c-4262-b008-94b4cf33ad47')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d558e189-341c-4262-b008-94b4cf33ad47 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d558e189-341c-4262-b008-94b4cf33ad47');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7f96ea74-b28d-4ab7-b5c6-476731b163cc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f96ea74-b28d-4ab7-b5c6-476731b163cc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7f96ea74-b28d-4ab7-b5c6-476731b163cc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_31bc0834-39d1-48df-95c7-f27216e355fb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_31bc0834-39d1-48df-95c7-f27216e355fb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"max_depth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          10,\n          12,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012344267996967364,\n        \"min\": 0.705,\n        \"max\": 0.745,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.705,\n          0.73,\n          0.71\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"execution_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.369804591591311,\n        \"min\": 0.04167914390563965,\n        \"max\": 4.218148231506348,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          2.820723056793213,\n          2.8902955055236816,\n          0.04167914390563965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kedalaman Pohon dengan AdaBoot split Data 80:20**"
      ],
      "metadata": {
        "id": "y0cOTDQRGp7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dan Preprocessing sama seperti sebelumnya...\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Fungsi entropy, gain_ratio_numeric, gain_ratio_categorical, best_split, build_tree, predict\n",
        "# Sama seperti kode yang kamu sudah berikan, saya anggap sudah ada di sini\n",
        "\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes, max_depth, current_depth=0):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0 or current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "# Fungsi untuk membuat weighted sample dari data (berdasarkan bobot)\n",
        "def weighted_sample(data, weights):\n",
        "    n_samples = len(data)\n",
        "    indices = np.random.choice(n_samples, size=n_samples, replace=True, p=weights)\n",
        "    return data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "# Implementasi AdaBoost\n",
        "def adaBoost(train_data, test_data, attributes, target='class', n_estimators=10, max_depth=3):\n",
        "    n_samples = len(train_data)\n",
        "    weights = np.ones(n_samples) / n_samples  # bobot awal sama rata\n",
        "    trees = []\n",
        "    alphas = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        # Sampling data sesuai bobot\n",
        "        sample_data = weighted_sample(train_data, weights)\n",
        "\n",
        "        # Bangun pohon\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "\n",
        "        # Prediksi pada data training asli untuk hitung error\n",
        "        preds = np.array([predict(tree, row) for _, row in train_data.iterrows()])\n",
        "        actual = train_data[target].values\n",
        "\n",
        "        # Hitung error weighted\n",
        "        incorrect = (preds != actual).astype(int)\n",
        "        error = np.sum(weights * incorrect) / np.sum(weights)\n",
        "        if error > 0.5:\n",
        "            # Jika error lebih besar dari 0.5, skip learner ini\n",
        "            continue\n",
        "        if error == 0:\n",
        "            error = 1e-10  # untuk menghindari pembagian dengan nol\n",
        "\n",
        "        # Hitung alpha (bobot learner)\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "\n",
        "        # Update bobot sampel\n",
        "        weights = weights * np.exp(-alpha * (1 - 2*incorrect))  # benar=0, salah=1 → -alpha*(1-2*1)=-alpha*(-1)=+alpha\n",
        "        weights = weights / np.sum(weights)  # normalisasi\n",
        "\n",
        "        trees.append(tree)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "        print(f\"Pohon ke-{m+1}, max_depth={max_depth}, error={error:.4f}, alpha={alpha:.4f}\")\n",
        "\n",
        "    # Prediksi ensemble pada test set dengan weighted voting\n",
        "    def predict_ensemble(sample):\n",
        "        class_votes = {}\n",
        "        for tree, alpha in zip(trees, alphas):\n",
        "            pred = predict(tree, sample)\n",
        "            class_votes[pred] = class_votes.get(pred, 0) + alpha\n",
        "        # Pilih kelas dengan total alpha terbesar\n",
        "        return max(class_votes, key=class_votes.get)\n",
        "\n",
        "    predictions = [predict_ensemble(row) for _, row in test_data.iterrows()]\n",
        "    accuracy = accuracy_score(test_data[target], predictions)\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    return accuracy, exec_time, n_estimators, max_depth\n",
        "\n",
        "# Jalankan AdaBoost dengan kedalaman pohon 1 sampai 15\n",
        "results = []\n",
        "for depth in range(1, 16):\n",
        "    acc, t_exec, n_trees, max_d = adaBoost(train_data, test_data, attributes, target='class',\n",
        "                                           n_estimators=10, max_depth=depth)\n",
        "    results.append((max_d, acc, t_exec))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['max_depth', 'accuracy', 'execution_time'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVVLfMgFGqHg",
        "outputId": "e0b074cc-9f68-4dc3-dedd-6a71f14a8045"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pohon ke-1, max_depth=1, error=0.3013, alpha=0.4207\n",
            "Pohon ke-2, max_depth=1, error=0.3246, alpha=0.3663\n",
            "Pohon ke-3, max_depth=1, error=0.4309, alpha=0.1391\n",
            "Pohon ke-4, max_depth=1, error=0.4450, alpha=0.1105\n",
            "Pohon ke-5, max_depth=1, error=0.4695, alpha=0.0611\n",
            "Pohon ke-6, max_depth=1, error=0.4742, alpha=0.0517\n",
            "Pohon ke-7, max_depth=1, error=0.4882, alpha=0.0236\n",
            "Pohon ke-8, max_depth=1, error=0.4673, alpha=0.0655\n",
            "Pohon ke-9, max_depth=1, error=0.4730, alpha=0.0540\n",
            "Pohon ke-10, max_depth=1, error=0.4307, alpha=0.1395\n",
            "Pohon ke-1, max_depth=2, error=0.2800, alpha=0.4722\n",
            "Pohon ke-2, max_depth=2, error=0.3413, alpha=0.3288\n",
            "Pohon ke-3, max_depth=2, error=0.4252, alpha=0.1506\n",
            "Pohon ke-4, max_depth=2, error=0.4565, alpha=0.0873\n",
            "Pohon ke-5, max_depth=2, error=0.4672, alpha=0.0656\n",
            "Pohon ke-6, max_depth=2, error=0.4417, alpha=0.1172\n",
            "Pohon ke-7, max_depth=2, error=0.4216, alpha=0.1581\n",
            "Pohon ke-8, max_depth=2, error=0.4279, alpha=0.1453\n",
            "Pohon ke-9, max_depth=2, error=0.4573, alpha=0.0855\n",
            "Pohon ke-10, max_depth=2, error=0.4688, alpha=0.0626\n",
            "Pohon ke-1, max_depth=3, error=0.2738, alpha=0.4878\n",
            "Pohon ke-2, max_depth=3, error=0.3228, alpha=0.3705\n",
            "Pohon ke-3, max_depth=3, error=0.4326, alpha=0.1356\n",
            "Pohon ke-4, max_depth=3, error=0.4058, alpha=0.1908\n",
            "Pohon ke-5, max_depth=3, error=0.4192, alpha=0.1631\n",
            "Pohon ke-6, max_depth=3, error=0.4608, alpha=0.0786\n",
            "Pohon ke-7, max_depth=3, error=0.4327, alpha=0.1354\n",
            "Pohon ke-8, max_depth=3, error=0.4397, alpha=0.1212\n",
            "Pohon ke-9, max_depth=3, error=0.4303, alpha=0.1402\n",
            "Pohon ke-10, max_depth=3, error=0.4380, alpha=0.1246\n",
            "Pohon ke-1, max_depth=4, error=0.2763, alpha=0.4816\n",
            "Pohon ke-2, max_depth=4, error=0.3090, alpha=0.4024\n",
            "Pohon ke-3, max_depth=4, error=0.4008, alpha=0.2010\n",
            "Pohon ke-4, max_depth=4, error=0.3875, alpha=0.2290\n",
            "Pohon ke-5, max_depth=4, error=0.4130, alpha=0.1758\n",
            "Pohon ke-6, max_depth=4, error=0.4189, alpha=0.1637\n",
            "Pohon ke-7, max_depth=4, error=0.4397, alpha=0.1213\n",
            "Pohon ke-8, max_depth=4, error=0.4174, alpha=0.1667\n",
            "Pohon ke-9, max_depth=4, error=0.4448, alpha=0.1109\n",
            "Pohon ke-10, max_depth=4, error=0.4113, alpha=0.1793\n",
            "Pohon ke-1, max_depth=5, error=0.2925, alpha=0.4416\n",
            "Pohon ke-2, max_depth=5, error=0.2974, alpha=0.4299\n",
            "Pohon ke-3, max_depth=5, error=0.3315, alpha=0.3507\n",
            "Pohon ke-4, max_depth=5, error=0.3508, alpha=0.3077\n",
            "Pohon ke-5, max_depth=5, error=0.3859, alpha=0.2322\n",
            "Pohon ke-6, max_depth=5, error=0.4232, alpha=0.1548\n",
            "Pohon ke-7, max_depth=5, error=0.4025, alpha=0.1974\n",
            "Pohon ke-8, max_depth=5, error=0.4297, alpha=0.1416\n",
            "Pohon ke-9, max_depth=5, error=0.4165, alpha=0.1685\n",
            "Pohon ke-10, max_depth=5, error=0.3806, alpha=0.2434\n",
            "Pohon ke-1, max_depth=6, error=0.2600, alpha=0.5230\n",
            "Pohon ke-2, max_depth=6, error=0.2803, alpha=0.4714\n",
            "Pohon ke-3, max_depth=6, error=0.3938, alpha=0.2156\n",
            "Pohon ke-4, max_depth=6, error=0.3922, alpha=0.2190\n",
            "Pohon ke-5, max_depth=6, error=0.3918, alpha=0.2199\n",
            "Pohon ke-6, max_depth=6, error=0.3896, alpha=0.2245\n",
            "Pohon ke-7, max_depth=6, error=0.3420, alpha=0.3273\n",
            "Pohon ke-8, max_depth=6, error=0.4228, alpha=0.1557\n",
            "Pohon ke-9, max_depth=6, error=0.4350, alpha=0.1307\n",
            "Pohon ke-10, max_depth=6, error=0.3996, alpha=0.2035\n",
            "Pohon ke-1, max_depth=7, error=0.2575, alpha=0.5295\n",
            "Pohon ke-2, max_depth=7, error=0.2449, alpha=0.5630\n",
            "Pohon ke-3, max_depth=7, error=0.3725, alpha=0.2608\n",
            "Pohon ke-4, max_depth=7, error=0.3248, alpha=0.3659\n",
            "Pohon ke-5, max_depth=7, error=0.3628, alpha=0.2817\n",
            "Pohon ke-6, max_depth=7, error=0.3929, alpha=0.2175\n",
            "Pohon ke-7, max_depth=7, error=0.4174, alpha=0.1668\n",
            "Pohon ke-8, max_depth=7, error=0.3743, alpha=0.2570\n",
            "Pohon ke-9, max_depth=7, error=0.3638, alpha=0.2795\n",
            "Pohon ke-10, max_depth=7, error=0.3941, alpha=0.2151\n",
            "Pohon ke-1, max_depth=8, error=0.2125, alpha=0.6550\n",
            "Pohon ke-2, max_depth=8, error=0.2609, alpha=0.5206\n",
            "Pohon ke-3, max_depth=8, error=0.3303, alpha=0.3535\n",
            "Pohon ke-4, max_depth=8, error=0.3068, alpha=0.4075\n",
            "Pohon ke-5, max_depth=8, error=0.3285, alpha=0.3574\n",
            "Pohon ke-6, max_depth=8, error=0.3065, alpha=0.4083\n",
            "Pohon ke-7, max_depth=8, error=0.3880, alpha=0.2278\n",
            "Pohon ke-8, max_depth=8, error=0.3840, alpha=0.2363\n",
            "Pohon ke-9, max_depth=8, error=0.3433, alpha=0.3242\n",
            "Pohon ke-10, max_depth=8, error=0.3361, alpha=0.3405\n",
            "Pohon ke-1, max_depth=9, error=0.2300, alpha=0.6042\n",
            "Pohon ke-2, max_depth=9, error=0.2529, alpha=0.5416\n",
            "Pohon ke-3, max_depth=9, error=0.2850, alpha=0.4600\n",
            "Pohon ke-4, max_depth=9, error=0.3199, alpha=0.3772\n",
            "Pohon ke-5, max_depth=9, error=0.3720, alpha=0.2618\n",
            "Pohon ke-6, max_depth=9, error=0.3472, alpha=0.3158\n",
            "Pohon ke-7, max_depth=9, error=0.3109, alpha=0.3980\n",
            "Pohon ke-8, max_depth=9, error=0.3178, alpha=0.3819\n",
            "Pohon ke-9, max_depth=9, error=0.3962, alpha=0.2106\n",
            "Pohon ke-10, max_depth=9, error=0.3512, alpha=0.3069\n",
            "Pohon ke-1, max_depth=10, error=0.2188, alpha=0.6365\n",
            "Pohon ke-2, max_depth=10, error=0.2643, alpha=0.5118\n",
            "Pohon ke-3, max_depth=10, error=0.2816, alpha=0.4682\n",
            "Pohon ke-4, max_depth=10, error=0.3057, alpha=0.4100\n",
            "Pohon ke-5, max_depth=10, error=0.3358, alpha=0.3410\n",
            "Pohon ke-6, max_depth=10, error=0.3292, alpha=0.3558\n",
            "Pohon ke-7, max_depth=10, error=0.3173, alpha=0.3830\n",
            "Pohon ke-8, max_depth=10, error=0.2105, alpha=0.6610\n",
            "Pohon ke-9, max_depth=10, error=0.2529, alpha=0.5415\n",
            "Pohon ke-10, max_depth=10, error=0.4247, alpha=0.1518\n",
            "Pohon ke-1, max_depth=11, error=0.1813, alpha=0.7540\n",
            "Pohon ke-2, max_depth=11, error=0.2378, alpha=0.5825\n",
            "Pohon ke-3, max_depth=11, error=0.2582, alpha=0.5277\n",
            "Pohon ke-4, max_depth=11, error=0.2724, alpha=0.4913\n",
            "Pohon ke-5, max_depth=11, error=0.3138, alpha=0.3911\n",
            "Pohon ke-6, max_depth=11, error=0.3077, alpha=0.4054\n",
            "Pohon ke-7, max_depth=11, error=0.2966, alpha=0.4317\n",
            "Pohon ke-8, max_depth=11, error=0.2702, alpha=0.4968\n",
            "Pohon ke-9, max_depth=11, error=0.3187, alpha=0.3799\n",
            "Pohon ke-10, max_depth=11, error=0.2752, alpha=0.4842\n",
            "Pohon ke-1, max_depth=12, error=0.1950, alpha=0.7089\n",
            "Pohon ke-2, max_depth=12, error=0.1777, alpha=0.7662\n",
            "Pohon ke-3, max_depth=12, error=0.2696, alpha=0.4983\n",
            "Pohon ke-4, max_depth=12, error=0.2419, alpha=0.5713\n",
            "Pohon ke-5, max_depth=12, error=0.1968, alpha=0.7032\n",
            "Pohon ke-6, max_depth=12, error=0.3034, alpha=0.4155\n",
            "Pohon ke-7, max_depth=12, error=0.1871, alpha=0.7344\n",
            "Pohon ke-8, max_depth=12, error=0.2202, alpha=0.6322\n",
            "Pohon ke-9, max_depth=12, error=0.2356, alpha=0.5884\n",
            "Pohon ke-10, max_depth=12, error=0.2185, alpha=0.6373\n",
            "Pohon ke-1, max_depth=13, error=0.1538, alpha=0.8527\n",
            "Pohon ke-2, max_depth=13, error=0.1729, alpha=0.7827\n",
            "Pohon ke-3, max_depth=13, error=0.2183, alpha=0.6378\n",
            "Pohon ke-4, max_depth=13, error=0.1897, alpha=0.7260\n",
            "Pohon ke-5, max_depth=13, error=0.1535, alpha=0.8539\n",
            "Pohon ke-6, max_depth=13, error=0.1527, alpha=0.8569\n",
            "Pohon ke-7, max_depth=13, error=0.1849, alpha=0.7418\n",
            "Pohon ke-8, max_depth=13, error=0.1780, alpha=0.7651\n",
            "Pohon ke-9, max_depth=13, error=0.2354, alpha=0.5890\n",
            "Pohon ke-10, max_depth=13, error=0.1519, alpha=0.8599\n",
            "Pohon ke-1, max_depth=14, error=0.1850, alpha=0.7414\n",
            "Pohon ke-2, max_depth=14, error=0.1797, alpha=0.7593\n",
            "Pohon ke-3, max_depth=14, error=0.1997, alpha=0.6941\n",
            "Pohon ke-4, max_depth=14, error=0.2159, alpha=0.6449\n",
            "Pohon ke-5, max_depth=14, error=0.2044, alpha=0.6795\n",
            "Pohon ke-6, max_depth=14, error=0.1869, alpha=0.7352\n",
            "Pohon ke-7, max_depth=14, error=0.2053, alpha=0.6768\n",
            "Pohon ke-8, max_depth=14, error=0.2074, alpha=0.6704\n",
            "Pohon ke-9, max_depth=14, error=0.2469, alpha=0.5575\n",
            "Pohon ke-10, max_depth=14, error=0.1516, alpha=0.8610\n",
            "Pohon ke-1, max_depth=15, error=0.1300, alpha=0.9505\n",
            "Pohon ke-2, max_depth=15, error=0.2110, alpha=0.6593\n",
            "Pohon ke-3, max_depth=15, error=0.1830, alpha=0.7480\n",
            "Pohon ke-4, max_depth=15, error=0.2135, alpha=0.6521\n",
            "Pohon ke-5, max_depth=15, error=0.1686, alpha=0.7979\n",
            "Pohon ke-6, max_depth=15, error=0.2358, alpha=0.5878\n",
            "Pohon ke-7, max_depth=15, error=0.2154, alpha=0.6462\n",
            "Pohon ke-8, max_depth=15, error=0.1643, alpha=0.8132\n",
            "Pohon ke-9, max_depth=15, error=0.2093, alpha=0.6644\n",
            "Pohon ke-10, max_depth=15, error=0.1876, alpha=0.7329\n",
            "    max_depth  accuracy  execution_time\n",
            "0           1     0.705        0.884032\n",
            "1           2     0.735        4.639729\n",
            "2           3     0.715        5.744788\n",
            "3           4     0.730       10.918941\n",
            "4           5     0.775        7.024212\n",
            "5           6     0.750        8.023338\n",
            "6           7     0.750       10.630506\n",
            "7           8     0.730       14.103135\n",
            "8           9     0.720       15.798095\n",
            "9          10     0.720       19.775284\n",
            "10         11     0.750       20.510487\n",
            "11         12     0.745       25.991825\n",
            "12         13     0.755       31.340531\n",
            "13         14     0.755       28.108920\n",
            "14         15     0.750       30.544817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Kedalaman Pohon AdaBoots dengan split Data 70:30**"
      ],
      "metadata": {
        "id": "n1rAo88mISSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dan Preprocessing sama seperti sebelumnya...\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Fungsi entropy, gain_ratio_numeric, gain_ratio_categorical, best_split, build_tree, predict\n",
        "# Sama seperti kode yang kamu sudah berikan, saya anggap sudah ada di sini\n",
        "\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes, max_depth, current_depth=0):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0 or current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "# Fungsi untuk membuat weighted sample dari data (berdasarkan bobot)\n",
        "def weighted_sample(data, weights):\n",
        "    n_samples = len(data)\n",
        "    indices = np.random.choice(n_samples, size=n_samples, replace=True, p=weights)\n",
        "    return data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "# Implementasi AdaBoost\n",
        "def adaBoost(train_data, test_data, attributes, target='class', n_estimators=10, max_depth=3):\n",
        "    n_samples = len(train_data)\n",
        "    weights = np.ones(n_samples) / n_samples  # bobot awal sama rata\n",
        "    trees = []\n",
        "    alphas = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        # Sampling data sesuai bobot\n",
        "        sample_data = weighted_sample(train_data, weights)\n",
        "\n",
        "        # Bangun pohon\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "\n",
        "        # Prediksi pada data training asli untuk hitung error\n",
        "        preds = np.array([predict(tree, row) for _, row in train_data.iterrows()])\n",
        "        actual = train_data[target].values\n",
        "\n",
        "        # Hitung error weighted\n",
        "        incorrect = (preds != actual).astype(int)\n",
        "        error = np.sum(weights * incorrect) / np.sum(weights)\n",
        "        if error > 0.5:\n",
        "            # Jika error lebih besar dari 0.5, skip learner ini\n",
        "            continue\n",
        "        if error == 0:\n",
        "            error = 1e-10  # untuk menghindari pembagian dengan nol\n",
        "\n",
        "        # Hitung alpha (bobot learner)\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "\n",
        "        # Update bobot sampel\n",
        "        weights = weights * np.exp(-alpha * (1 - 2*incorrect))  # benar=0, salah=1 → -alpha*(1-2*1)=-alpha*(-1)=+alpha\n",
        "        weights = weights / np.sum(weights)  # normalisasi\n",
        "\n",
        "        trees.append(tree)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "        print(f\"Pohon ke-{m+1}, max_depth={max_depth}, error={error:.4f}, alpha={alpha:.4f}\")\n",
        "\n",
        "    # Prediksi ensemble pada test set dengan weighted voting\n",
        "    def predict_ensemble(sample):\n",
        "        class_votes = {}\n",
        "        for tree, alpha in zip(trees, alphas):\n",
        "            pred = predict(tree, sample)\n",
        "            class_votes[pred] = class_votes.get(pred, 0) + alpha\n",
        "        # Pilih kelas dengan total alpha terbesar\n",
        "        return max(class_votes, key=class_votes.get)\n",
        "\n",
        "    predictions = [predict_ensemble(row) for _, row in test_data.iterrows()]\n",
        "    accuracy = accuracy_score(test_data[target], predictions)\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    return accuracy, exec_time, n_estimators, max_depth\n",
        "\n",
        "# Jalankan AdaBoost dengan kedalaman pohon 1 sampai 15\n",
        "results = []\n",
        "for depth in range(1, 16):\n",
        "    acc, t_exec, n_trees, max_d = adaBoost(train_data, test_data, attributes, target='class',\n",
        "                                           n_estimators=10, max_depth=depth)\n",
        "    results.append((max_d, acc, t_exec))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['max_depth', 'accuracy', 'execution_time'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwBb02u1ISa3",
        "outputId": "121ad408-a31c-4831-a056-48bf8a5914b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pohon ke-1, max_depth=1, error=0.2986, alpha=0.4271\n",
            "Pohon ke-2, max_depth=1, error=0.4655, alpha=0.0692\n",
            "Pohon ke-3, max_depth=1, error=0.3321, alpha=0.3493\n",
            "Pohon ke-4, max_depth=1, error=0.4264, alpha=0.1483\n",
            "Pohon ke-5, max_depth=1, error=0.4504, alpha=0.0996\n",
            "Pohon ke-6, max_depth=1, error=0.4623, alpha=0.0755\n",
            "Pohon ke-7, max_depth=1, error=0.4362, alpha=0.1284\n",
            "Pohon ke-8, max_depth=1, error=0.4567, alpha=0.0867\n",
            "Pohon ke-9, max_depth=1, error=0.4785, alpha=0.0429\n",
            "Pohon ke-10, max_depth=1, error=0.4824, alpha=0.0353\n",
            "Pohon ke-1, max_depth=2, error=0.2800, alpha=0.4722\n",
            "Pohon ke-2, max_depth=2, error=0.3410, alpha=0.3295\n",
            "Pohon ke-3, max_depth=2, error=0.4370, alpha=0.1267\n",
            "Pohon ke-4, max_depth=2, error=0.4440, alpha=0.1124\n",
            "Pohon ke-5, max_depth=2, error=0.4421, alpha=0.1164\n",
            "Pohon ke-6, max_depth=2, error=0.4093, alpha=0.1835\n",
            "Pohon ke-7, max_depth=2, error=0.4623, alpha=0.0756\n",
            "Pohon ke-8, max_depth=2, error=0.4167, alpha=0.1681\n",
            "Pohon ke-9, max_depth=2, error=0.4625, alpha=0.0752\n",
            "Pohon ke-10, max_depth=2, error=0.4456, alpha=0.1092\n",
            "Pohon ke-1, max_depth=3, error=0.2443, alpha=0.5647\n",
            "Pohon ke-2, max_depth=3, error=0.3386, alpha=0.3347\n",
            "Pohon ke-3, max_depth=3, error=0.3847, alpha=0.2349\n",
            "Pohon ke-4, max_depth=3, error=0.4020, alpha=0.1987\n",
            "Pohon ke-5, max_depth=3, error=0.4474, alpha=0.1055\n",
            "Pohon ke-6, max_depth=3, error=0.4493, alpha=0.1017\n",
            "Pohon ke-7, max_depth=3, error=0.4549, alpha=0.0904\n",
            "Pohon ke-8, max_depth=3, error=0.4050, alpha=0.1923\n",
            "Pohon ke-9, max_depth=3, error=0.4645, alpha=0.0711\n",
            "Pohon ke-10, max_depth=3, error=0.4169, alpha=0.1678\n",
            "Pohon ke-1, max_depth=4, error=0.2614, alpha=0.5193\n",
            "Pohon ke-2, max_depth=4, error=0.2947, alpha=0.4363\n",
            "Pohon ke-3, max_depth=4, error=0.3823, alpha=0.2399\n",
            "Pohon ke-4, max_depth=4, error=0.3726, alpha=0.2604\n",
            "Pohon ke-5, max_depth=4, error=0.4148, alpha=0.1720\n",
            "Pohon ke-6, max_depth=4, error=0.4048, alpha=0.1927\n",
            "Pohon ke-7, max_depth=4, error=0.4126, alpha=0.1766\n",
            "Pohon ke-8, max_depth=4, error=0.4414, alpha=0.1177\n",
            "Pohon ke-9, max_depth=4, error=0.4552, alpha=0.0898\n",
            "Pohon ke-10, max_depth=4, error=0.4136, alpha=0.1745\n",
            "Pohon ke-1, max_depth=5, error=0.2543, alpha=0.5379\n",
            "Pohon ke-2, max_depth=5, error=0.3149, alpha=0.3887\n",
            "Pohon ke-3, max_depth=5, error=0.3730, alpha=0.2597\n",
            "Pohon ke-4, max_depth=5, error=0.4168, alpha=0.1679\n",
            "Pohon ke-5, max_depth=5, error=0.3949, alpha=0.2134\n",
            "Pohon ke-6, max_depth=5, error=0.3529, alpha=0.3031\n",
            "Pohon ke-7, max_depth=5, error=0.3727, alpha=0.2604\n",
            "Pohon ke-8, max_depth=5, error=0.3693, alpha=0.2676\n",
            "Pohon ke-9, max_depth=5, error=0.4071, alpha=0.1879\n",
            "Pohon ke-10, max_depth=5, error=0.4112, alpha=0.1794\n",
            "Pohon ke-1, max_depth=6, error=0.2314, alpha=0.6001\n",
            "Pohon ke-2, max_depth=6, error=0.2928, alpha=0.4410\n",
            "Pohon ke-3, max_depth=6, error=0.3661, alpha=0.2745\n",
            "Pohon ke-4, max_depth=6, error=0.3017, alpha=0.4197\n",
            "Pohon ke-5, max_depth=6, error=0.3477, alpha=0.3145\n",
            "Pohon ke-6, max_depth=6, error=0.4116, alpha=0.1787\n",
            "Pohon ke-7, max_depth=6, error=0.3780, alpha=0.2490\n",
            "Pohon ke-8, max_depth=6, error=0.4040, alpha=0.1944\n",
            "Pohon ke-9, max_depth=6, error=0.3962, alpha=0.2107\n",
            "Pohon ke-10, max_depth=6, error=0.3907, alpha=0.2222\n",
            "Pohon ke-1, max_depth=7, error=0.2343, alpha=0.5921\n",
            "Pohon ke-2, max_depth=7, error=0.3133, alpha=0.3923\n",
            "Pohon ke-3, max_depth=7, error=0.2518, alpha=0.5444\n",
            "Pohon ke-4, max_depth=7, error=0.3478, alpha=0.3144\n",
            "Pohon ke-5, max_depth=7, error=0.3431, alpha=0.3247\n",
            "Pohon ke-6, max_depth=7, error=0.3694, alpha=0.2674\n",
            "Pohon ke-7, max_depth=7, error=0.3825, alpha=0.2395\n",
            "Pohon ke-8, max_depth=7, error=0.3468, alpha=0.3165\n",
            "Pohon ke-9, max_depth=7, error=0.4040, alpha=0.1944\n",
            "Pohon ke-10, max_depth=7, error=0.3943, alpha=0.2147\n",
            "Pohon ke-1, max_depth=8, error=0.2057, alpha=0.6755\n",
            "Pohon ke-2, max_depth=8, error=0.2397, alpha=0.5772\n",
            "Pohon ke-3, max_depth=8, error=0.2977, alpha=0.4291\n",
            "Pohon ke-4, max_depth=8, error=0.3278, alpha=0.3591\n",
            "Pohon ke-5, max_depth=8, error=0.3031, alpha=0.4164\n",
            "Pohon ke-6, max_depth=8, error=0.3871, alpha=0.2298\n",
            "Pohon ke-7, max_depth=8, error=0.3368, alpha=0.3388\n",
            "Pohon ke-8, max_depth=8, error=0.2654, alpha=0.5090\n",
            "Pohon ke-9, max_depth=8, error=0.3628, alpha=0.2816\n",
            "Pohon ke-10, max_depth=8, error=0.4053, alpha=0.1917\n",
            "Pohon ke-1, max_depth=9, error=0.2143, alpha=0.6496\n",
            "Pohon ke-2, max_depth=9, error=0.2721, alpha=0.4919\n",
            "Pohon ke-3, max_depth=9, error=0.3431, alpha=0.3249\n",
            "Pohon ke-4, max_depth=9, error=0.3186, alpha=0.3801\n",
            "Pohon ke-5, max_depth=9, error=0.3143, alpha=0.3901\n",
            "Pohon ke-6, max_depth=9, error=0.3346, alpha=0.3438\n",
            "Pohon ke-7, max_depth=9, error=0.4018, alpha=0.1990\n",
            "Pohon ke-8, max_depth=9, error=0.3409, alpha=0.3297\n",
            "Pohon ke-9, max_depth=9, error=0.3517, alpha=0.3058\n",
            "Pohon ke-10, max_depth=9, error=0.3300, alpha=0.3541\n",
            "Pohon ke-1, max_depth=10, error=0.2286, alpha=0.6082\n",
            "Pohon ke-2, max_depth=10, error=0.1839, alpha=0.7450\n",
            "Pohon ke-3, max_depth=10, error=0.2897, alpha=0.4484\n",
            "Pohon ke-4, max_depth=10, error=0.2837, alpha=0.4631\n",
            "Pohon ke-5, max_depth=10, error=0.2195, alpha=0.6342\n",
            "Pohon ke-6, max_depth=10, error=0.2899, alpha=0.4480\n",
            "Pohon ke-7, max_depth=10, error=0.2465, alpha=0.5586\n",
            "Pohon ke-8, max_depth=10, error=0.2325, alpha=0.5971\n",
            "Pohon ke-9, max_depth=10, error=0.2646, alpha=0.5110\n",
            "Pohon ke-10, max_depth=10, error=0.2847, alpha=0.4606\n",
            "Pohon ke-1, max_depth=11, error=0.2014, alpha=0.6887\n",
            "Pohon ke-2, max_depth=11, error=0.2097, alpha=0.6634\n",
            "Pohon ke-3, max_depth=11, error=0.2324, alpha=0.5975\n",
            "Pohon ke-4, max_depth=11, error=0.1786, alpha=0.7630\n",
            "Pohon ke-5, max_depth=11, error=0.2060, alpha=0.6745\n",
            "Pohon ke-6, max_depth=11, error=0.1832, alpha=0.7475\n",
            "Pohon ke-7, max_depth=11, error=0.2755, alpha=0.4833\n",
            "Pohon ke-8, max_depth=11, error=0.2913, alpha=0.4445\n",
            "Pohon ke-9, max_depth=11, error=0.1688, alpha=0.7969\n",
            "Pohon ke-10, max_depth=11, error=0.2585, alpha=0.5268\n",
            "Pohon ke-1, max_depth=12, error=0.1914, alpha=0.7204\n",
            "Pohon ke-2, max_depth=12, error=0.1800, alpha=0.7580\n",
            "Pohon ke-3, max_depth=12, error=0.3103, alpha=0.3993\n",
            "Pohon ke-4, max_depth=12, error=0.2610, alpha=0.5205\n",
            "Pohon ke-5, max_depth=12, error=0.2374, alpha=0.5835\n",
            "Pohon ke-6, max_depth=12, error=0.2604, alpha=0.5219\n",
            "Pohon ke-7, max_depth=12, error=0.2168, alpha=0.6421\n",
            "Pohon ke-8, max_depth=12, error=0.2537, alpha=0.5394\n",
            "Pohon ke-9, max_depth=12, error=0.2317, alpha=0.5994\n",
            "Pohon ke-10, max_depth=12, error=0.2578, alpha=0.5288\n",
            "Pohon ke-1, max_depth=13, error=0.1757, alpha=0.7728\n",
            "Pohon ke-2, max_depth=13, error=0.1856, alpha=0.7396\n",
            "Pohon ke-3, max_depth=13, error=0.1333, alpha=0.9359\n",
            "Pohon ke-4, max_depth=13, error=0.1754, alpha=0.7740\n",
            "Pohon ke-5, max_depth=13, error=0.2183, alpha=0.6378\n",
            "Pohon ke-6, max_depth=13, error=0.2166, alpha=0.6427\n",
            "Pohon ke-7, max_depth=13, error=0.1645, alpha=0.8125\n",
            "Pohon ke-8, max_depth=13, error=0.2135, alpha=0.6521\n",
            "Pohon ke-9, max_depth=13, error=0.2913, alpha=0.4446\n",
            "Pohon ke-10, max_depth=13, error=0.1799, alpha=0.7587\n",
            "Pohon ke-1, max_depth=14, error=0.1614, alpha=0.8238\n",
            "Pohon ke-2, max_depth=14, error=0.2049, alpha=0.6781\n",
            "Pohon ke-3, max_depth=14, error=0.1506, alpha=0.8649\n",
            "Pohon ke-4, max_depth=14, error=0.1689, alpha=0.7968\n",
            "Pohon ke-5, max_depth=14, error=0.1812, alpha=0.7542\n",
            "Pohon ke-6, max_depth=14, error=0.1992, alpha=0.6956\n",
            "Pohon ke-7, max_depth=14, error=0.2009, alpha=0.6902\n",
            "Pohon ke-8, max_depth=14, error=0.1886, alpha=0.7295\n",
            "Pohon ke-9, max_depth=14, error=0.1946, alpha=0.7101\n",
            "Pohon ke-10, max_depth=14, error=0.1414, alpha=0.9017\n",
            "Pohon ke-1, max_depth=15, error=0.1714, alpha=0.7878\n",
            "Pohon ke-2, max_depth=15, error=0.1249, alpha=0.9736\n",
            "Pohon ke-3, max_depth=15, error=0.2373, alpha=0.5839\n",
            "Pohon ke-4, max_depth=15, error=0.1031, alpha=1.0815\n",
            "Pohon ke-5, max_depth=15, error=0.1231, alpha=0.9819\n",
            "Pohon ke-6, max_depth=15, error=0.1430, alpha=0.8954\n",
            "Pohon ke-7, max_depth=15, error=0.1741, alpha=0.7785\n",
            "Pohon ke-8, max_depth=15, error=0.1186, alpha=1.0028\n",
            "Pohon ke-9, max_depth=15, error=0.1507, alpha=0.8644\n",
            "Pohon ke-10, max_depth=15, error=0.1041, alpha=1.0764\n",
            "    max_depth  accuracy  execution_time\n",
            "0           1  0.720000        0.604156\n",
            "1           2  0.740000        1.298287\n",
            "2           3  0.756667        3.348100\n",
            "3           4  0.696667        4.048822\n",
            "4           5  0.740000        6.163064\n",
            "5           6  0.720000        8.059034\n",
            "6           7  0.713333       10.878062\n",
            "7           8  0.683333       12.831791\n",
            "8           9  0.710000       13.029630\n",
            "9          10  0.716667       18.878819\n",
            "10         11  0.690000       21.987944\n",
            "11         12  0.723333       21.294662\n",
            "12         13  0.730000       25.244630\n",
            "13         14  0.736667       25.569908\n",
            "14         15  0.690000       28.559199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Menggunakan Adaboot dengan split Data 80:20 Gunakan Program Ini karena ada kedalaman Pohon**"
      ],
      "metadata": {
        "id": "exjmEzkLNieH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dan preprocessing data\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Fungsi entropy, gain_ratio_numeric, gain_ratio_categorical, best_split, build_tree, predict\n",
        "\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes, max_depth, current_depth=0):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0 or current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            # Jika nilai kategori tidak ada di cabang, pilih kelas mayoritas leaf terdekat\n",
        "            return 0\n",
        "\n",
        "def weighted_sample(data, weights):\n",
        "    n_samples = len(data)\n",
        "    indices = np.random.choice(n_samples, size=n_samples, replace=True, p=weights)\n",
        "    return data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "def adaBoost(train_data, test_data, attributes, target='class', n_estimators=10, max_depth=3):\n",
        "    n_samples = len(train_data)\n",
        "    weights = np.ones(n_samples) / n_samples\n",
        "    trees = []\n",
        "    alphas = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_data = weighted_sample(train_data, weights)\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        preds = np.array([predict(tree, row) for _, row in train_data.iterrows()])\n",
        "        actual = train_data[target].values\n",
        "        incorrect = (preds != actual).astype(int)\n",
        "        error = np.sum(weights * incorrect) / np.sum(weights)\n",
        "        if error > 0.5:\n",
        "            continue\n",
        "        if error == 0:\n",
        "            error = 1e-10\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        weights = weights * np.exp(-alpha * (1 - 2*incorrect))\n",
        "        weights = weights / np.sum(weights)\n",
        "        trees.append(tree)\n",
        "        alphas.append(alpha)\n",
        "        print(f\"Pohon ke-{m+1}, max_depth={max_depth}, error={error:.4f}, alpha={alpha:.4f}\")\n",
        "\n",
        "    def predict_ensemble(sample):\n",
        "        class_votes = {}\n",
        "        for tree, alpha in zip(trees, alphas):\n",
        "            pred = predict(tree, sample)\n",
        "            class_votes[pred] = class_votes.get(pred, 0) + alpha\n",
        "        return max(class_votes, key=class_votes.get)\n",
        "\n",
        "    predictions = [predict_ensemble(row) for _, row in test_data.iterrows()]\n",
        "    accuracy = accuracy_score(test_data[target], predictions)\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    return accuracy, exec_time, n_estimators, max_depth, predictions\n",
        "\n",
        "# Jalankan AdaBoost dan tampilkan laporan klasifikasi + akurasi per max_depth\n",
        "results = []\n",
        "for depth in range(1, 16):\n",
        "    acc, t_exec, n_trees, max_d, preds = adaBoost(train_data, test_data, attributes, target='class',\n",
        "                                                  n_estimators=10, max_depth=depth)\n",
        "    print(f\"\\n=== max_depth = {max_d} ===\")\n",
        "    print(f\"Akurasi: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(\"Laporan Klasifikasi:\")\n",
        "    print(classification_report(test_data['class'], preds))\n",
        "    results.append((max_d, acc, t_exec))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['max_depth', 'accuracy', 'execution_time'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKNvWeUbNilp",
        "outputId": "08367e12-d27f-4477-847c-b9660d43ffd3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pohon ke-1, max_depth=1, error=0.3013, alpha=0.4207\n",
            "Pohon ke-2, max_depth=1, error=0.3246, alpha=0.3663\n",
            "Pohon ke-3, max_depth=1, error=0.4171, alpha=0.1673\n",
            "Pohon ke-4, max_depth=1, error=0.4390, alpha=0.1226\n",
            "Pohon ke-5, max_depth=1, error=0.4712, alpha=0.0576\n",
            "Pohon ke-6, max_depth=1, error=0.5000, alpha=0.0000\n",
            "Pohon ke-7, max_depth=1, error=0.4658, alpha=0.0686\n",
            "Pohon ke-9, max_depth=1, error=0.4716, alpha=0.0568\n",
            "Pohon ke-10, max_depth=1, error=0.4493, alpha=0.1018\n",
            "\n",
            "=== max_depth = 1 ===\n",
            "Akurasi: 0.7150 (71.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.03      0.07        59\n",
            "           1       0.71      1.00      0.83       141\n",
            "\n",
            "    accuracy                           0.71       200\n",
            "   macro avg       0.86      0.52      0.45       200\n",
            "weighted avg       0.80      0.71      0.61       200\n",
            "\n",
            "Pohon ke-1, max_depth=2, error=0.3013, alpha=0.4207\n",
            "Pohon ke-2, max_depth=2, error=0.3322, alpha=0.3492\n",
            "Pohon ke-3, max_depth=2, error=0.3771, alpha=0.2509\n",
            "Pohon ke-4, max_depth=2, error=0.4289, alpha=0.1433\n",
            "Pohon ke-5, max_depth=2, error=0.4185, alpha=0.1645\n",
            "Pohon ke-6, max_depth=2, error=0.4454, alpha=0.1096\n",
            "Pohon ke-7, max_depth=2, error=0.4934, alpha=0.0133\n",
            "Pohon ke-8, max_depth=2, error=0.4613, alpha=0.0776\n",
            "Pohon ke-9, max_depth=2, error=0.4293, alpha=0.1424\n",
            "Pohon ke-10, max_depth=2, error=0.4526, alpha=0.0952\n",
            "\n",
            "=== max_depth = 2 ===\n",
            "Akurasi: 0.7450 (74.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.15      0.26        59\n",
            "           1       0.74      0.99      0.85       141\n",
            "\n",
            "    accuracy                           0.74       200\n",
            "   macro avg       0.82      0.57      0.55       200\n",
            "weighted avg       0.78      0.74      0.67       200\n",
            "\n",
            "Pohon ke-1, max_depth=3, error=0.3038, alpha=0.4148\n",
            "Pohon ke-2, max_depth=3, error=0.3301, alpha=0.3539\n",
            "Pohon ke-3, max_depth=3, error=0.3646, alpha=0.2776\n",
            "Pohon ke-4, max_depth=3, error=0.3877, alpha=0.2286\n",
            "Pohon ke-5, max_depth=3, error=0.4426, alpha=0.1154\n",
            "Pohon ke-6, max_depth=3, error=0.4345, alpha=0.1317\n",
            "Pohon ke-7, max_depth=3, error=0.3982, alpha=0.2065\n",
            "Pohon ke-8, max_depth=3, error=0.4576, alpha=0.0849\n",
            "Pohon ke-9, max_depth=3, error=0.4497, alpha=0.1009\n",
            "Pohon ke-10, max_depth=3, error=0.4626, alpha=0.0749\n",
            "\n",
            "=== max_depth = 3 ===\n",
            "Akurasi: 0.7600 (76.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.46      0.53        59\n",
            "           1       0.80      0.89      0.84       141\n",
            "\n",
            "    accuracy                           0.76       200\n",
            "   macro avg       0.71      0.67      0.68       200\n",
            "weighted avg       0.75      0.76      0.75       200\n",
            "\n",
            "Pohon ke-1, max_depth=4, error=0.3050, alpha=0.4118\n",
            "Pohon ke-2, max_depth=4, error=0.2979, alpha=0.4287\n",
            "Pohon ke-3, max_depth=4, error=0.3673, alpha=0.2719\n",
            "Pohon ke-4, max_depth=4, error=0.3513, alpha=0.3066\n",
            "Pohon ke-5, max_depth=4, error=0.4062, alpha=0.1898\n",
            "Pohon ke-6, max_depth=4, error=0.3788, alpha=0.2472\n",
            "Pohon ke-7, max_depth=4, error=0.4082, alpha=0.1858\n",
            "Pohon ke-8, max_depth=4, error=0.4353, alpha=0.1302\n",
            "Pohon ke-9, max_depth=4, error=0.4362, alpha=0.1282\n",
            "Pohon ke-10, max_depth=4, error=0.4094, alpha=0.1833\n",
            "\n",
            "=== max_depth = 4 ===\n",
            "Akurasi: 0.7500 (75.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.39      0.48        59\n",
            "           1       0.78      0.90      0.84       141\n",
            "\n",
            "    accuracy                           0.75       200\n",
            "   macro avg       0.70      0.65      0.66       200\n",
            "weighted avg       0.73      0.75      0.73       200\n",
            "\n",
            "Pohon ke-1, max_depth=5, error=0.2913, alpha=0.4447\n",
            "Pohon ke-2, max_depth=5, error=0.2634, alpha=0.5142\n",
            "Pohon ke-3, max_depth=5, error=0.3897, alpha=0.2242\n",
            "Pohon ke-4, max_depth=5, error=0.4074, alpha=0.1873\n",
            "Pohon ke-5, max_depth=5, error=0.3829, alpha=0.2387\n",
            "Pohon ke-6, max_depth=5, error=0.4141, alpha=0.1736\n",
            "Pohon ke-7, max_depth=5, error=0.3970, alpha=0.2090\n",
            "Pohon ke-8, max_depth=5, error=0.3942, alpha=0.2149\n",
            "Pohon ke-9, max_depth=5, error=0.3758, alpha=0.2538\n",
            "Pohon ke-10, max_depth=5, error=0.4198, alpha=0.1619\n",
            "\n",
            "=== max_depth = 5 ===\n",
            "Akurasi: 0.7800 (78.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.54      0.59        59\n",
            "           1       0.82      0.88      0.85       141\n",
            "\n",
            "    accuracy                           0.78       200\n",
            "   macro avg       0.74      0.71      0.72       200\n",
            "weighted avg       0.77      0.78      0.77       200\n",
            "\n",
            "Pohon ke-1, max_depth=6, error=0.2500, alpha=0.5493\n",
            "Pohon ke-2, max_depth=6, error=0.3100, alpha=0.4001\n",
            "Pohon ke-3, max_depth=6, error=0.3569, alpha=0.2944\n",
            "Pohon ke-4, max_depth=6, error=0.3974, alpha=0.2082\n",
            "Pohon ke-5, max_depth=6, error=0.4121, alpha=0.1777\n",
            "Pohon ke-6, max_depth=6, error=0.3982, alpha=0.2064\n",
            "Pohon ke-7, max_depth=6, error=0.4066, alpha=0.1890\n",
            "Pohon ke-8, max_depth=6, error=0.4273, alpha=0.1463\n",
            "Pohon ke-9, max_depth=6, error=0.3829, alpha=0.2387\n",
            "Pohon ke-10, max_depth=6, error=0.4134, alpha=0.1750\n",
            "\n",
            "=== max_depth = 6 ===\n",
            "Akurasi: 0.7000 (70.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.31      0.38        59\n",
            "           1       0.75      0.87      0.80       141\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.62      0.59      0.59       200\n",
            "weighted avg       0.67      0.70      0.68       200\n",
            "\n",
            "Pohon ke-1, max_depth=7, error=0.2538, alpha=0.5394\n",
            "Pohon ke-2, max_depth=7, error=0.2883, alpha=0.4518\n",
            "Pohon ke-3, max_depth=7, error=0.3217, alpha=0.3729\n",
            "Pohon ke-4, max_depth=7, error=0.3308, alpha=0.3523\n",
            "Pohon ke-5, max_depth=7, error=0.3136, alpha=0.3916\n",
            "Pohon ke-6, max_depth=7, error=0.3723, alpha=0.2611\n",
            "Pohon ke-7, max_depth=7, error=0.3778, alpha=0.2495\n",
            "Pohon ke-8, max_depth=7, error=0.3473, alpha=0.3154\n",
            "Pohon ke-9, max_depth=7, error=0.3574, alpha=0.2934\n",
            "Pohon ke-10, max_depth=7, error=0.3719, alpha=0.2621\n",
            "\n",
            "=== max_depth = 7 ===\n",
            "Akurasi: 0.7400 (74.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.51      0.54        59\n",
            "           1       0.80      0.84      0.82       141\n",
            "\n",
            "    accuracy                           0.74       200\n",
            "   macro avg       0.68      0.67      0.68       200\n",
            "weighted avg       0.73      0.74      0.74       200\n",
            "\n",
            "Pohon ke-1, max_depth=8, error=0.2663, alpha=0.5069\n",
            "Pohon ke-2, max_depth=8, error=0.2841, alpha=0.4622\n",
            "Pohon ke-3, max_depth=8, error=0.3228, alpha=0.3704\n",
            "Pohon ke-4, max_depth=8, error=0.2954, alpha=0.4347\n",
            "Pohon ke-5, max_depth=8, error=0.3627, alpha=0.2818\n",
            "Pohon ke-6, max_depth=8, error=0.3228, alpha=0.3704\n",
            "Pohon ke-7, max_depth=8, error=0.4066, alpha=0.1890\n",
            "Pohon ke-8, max_depth=8, error=0.3688, alpha=0.2688\n",
            "Pohon ke-9, max_depth=8, error=0.3434, alpha=0.3241\n",
            "Pohon ke-10, max_depth=8, error=0.2966, alpha=0.4318\n",
            "\n",
            "=== max_depth = 8 ===\n",
            "Akurasi: 0.7150 (71.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.42      0.47        59\n",
            "           1       0.78      0.84      0.81       141\n",
            "\n",
            "    accuracy                           0.71       200\n",
            "   macro avg       0.65      0.63      0.64       200\n",
            "weighted avg       0.70      0.71      0.71       200\n",
            "\n",
            "Pohon ke-1, max_depth=9, error=0.2013, alpha=0.6893\n",
            "Pohon ke-2, max_depth=9, error=0.2356, alpha=0.5886\n",
            "Pohon ke-3, max_depth=9, error=0.3200, alpha=0.3769\n",
            "Pohon ke-4, max_depth=9, error=0.2953, alpha=0.4350\n",
            "Pohon ke-5, max_depth=9, error=0.3473, alpha=0.3155\n",
            "Pohon ke-6, max_depth=9, error=0.2884, alpha=0.4516\n",
            "Pohon ke-7, max_depth=9, error=0.3019, alpha=0.4191\n",
            "Pohon ke-8, max_depth=9, error=0.3481, alpha=0.3137\n",
            "Pohon ke-9, max_depth=9, error=0.3821, alpha=0.2404\n",
            "Pohon ke-10, max_depth=9, error=0.3421, alpha=0.3269\n",
            "\n",
            "=== max_depth = 9 ===\n",
            "Akurasi: 0.7300 (73.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.47      0.51        59\n",
            "           1       0.79      0.84      0.81       141\n",
            "\n",
            "    accuracy                           0.73       200\n",
            "   macro avg       0.67      0.66      0.66       200\n",
            "weighted avg       0.72      0.73      0.72       200\n",
            "\n",
            "Pohon ke-1, max_depth=10, error=0.2013, alpha=0.6893\n",
            "Pohon ke-2, max_depth=10, error=0.2163, alpha=0.6437\n",
            "Pohon ke-3, max_depth=10, error=0.3055, alpha=0.4105\n",
            "Pohon ke-4, max_depth=10, error=0.2713, alpha=0.4939\n",
            "Pohon ke-5, max_depth=10, error=0.3228, alpha=0.3705\n",
            "Pohon ke-6, max_depth=10, error=0.3446, alpha=0.3215\n",
            "Pohon ke-7, max_depth=10, error=0.2855, alpha=0.4588\n",
            "Pohon ke-8, max_depth=10, error=0.3646, alpha=0.2778\n",
            "Pohon ke-9, max_depth=10, error=0.3541, alpha=0.3005\n",
            "Pohon ke-10, max_depth=10, error=0.3442, alpha=0.3224\n",
            "\n",
            "=== max_depth = 10 ===\n",
            "Akurasi: 0.6850 (68.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.39      0.42        59\n",
            "           1       0.76      0.81      0.78       141\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.61      0.60      0.60       200\n",
            "weighted avg       0.67      0.69      0.68       200\n",
            "\n",
            "Pohon ke-1, max_depth=11, error=0.2088, alpha=0.6662\n",
            "Pohon ke-2, max_depth=11, error=0.2457, alpha=0.5609\n",
            "Pohon ke-3, max_depth=11, error=0.2049, alpha=0.6780\n",
            "Pohon ke-4, max_depth=11, error=0.2131, alpha=0.6533\n",
            "Pohon ke-5, max_depth=11, error=0.2392, alpha=0.5787\n",
            "Pohon ke-6, max_depth=11, error=0.2062, alpha=0.6739\n",
            "Pohon ke-7, max_depth=11, error=0.3096, alpha=0.4011\n",
            "Pohon ke-8, max_depth=11, error=0.2512, alpha=0.5460\n",
            "Pohon ke-9, max_depth=11, error=0.2597, alpha=0.5236\n",
            "Pohon ke-10, max_depth=11, error=0.2897, alpha=0.4484\n",
            "\n",
            "=== max_depth = 11 ===\n",
            "Akurasi: 0.7100 (71.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.47      0.49        59\n",
            "           1       0.79      0.81      0.80       141\n",
            "\n",
            "    accuracy                           0.71       200\n",
            "   macro avg       0.65      0.64      0.64       200\n",
            "weighted avg       0.70      0.71      0.71       200\n",
            "\n",
            "Pohon ke-1, max_depth=12, error=0.2425, alpha=0.5695\n",
            "Pohon ke-2, max_depth=12, error=0.1651, alpha=0.8104\n",
            "Pohon ke-3, max_depth=12, error=0.2120, alpha=0.6566\n",
            "Pohon ke-4, max_depth=12, error=0.1852, alpha=0.7408\n",
            "Pohon ke-5, max_depth=12, error=0.2582, alpha=0.5278\n",
            "Pohon ke-6, max_depth=12, error=0.2264, alpha=0.6144\n",
            "Pohon ke-7, max_depth=12, error=0.1861, alpha=0.7377\n",
            "Pohon ke-8, max_depth=12, error=0.1963, alpha=0.7049\n",
            "Pohon ke-9, max_depth=12, error=0.2278, alpha=0.6103\n",
            "Pohon ke-10, max_depth=12, error=0.1370, alpha=0.9203\n",
            "\n",
            "=== max_depth = 12 ===\n",
            "Akurasi: 0.7050 (70.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.44      0.47        59\n",
            "           1       0.78      0.82      0.80       141\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.64      0.63      0.63       200\n",
            "weighted avg       0.70      0.70      0.70       200\n",
            "\n",
            "Pohon ke-1, max_depth=13, error=0.1988, alpha=0.6971\n",
            "Pohon ke-2, max_depth=13, error=0.1850, alpha=0.7414\n",
            "Pohon ke-3, max_depth=13, error=0.1794, alpha=0.7603\n",
            "Pohon ke-4, max_depth=13, error=0.2276, alpha=0.6108\n",
            "Pohon ke-5, max_depth=13, error=0.1443, alpha=0.8902\n",
            "Pohon ke-6, max_depth=13, error=0.2249, alpha=0.6187\n",
            "Pohon ke-7, max_depth=13, error=0.2007, alpha=0.6909\n",
            "Pohon ke-8, max_depth=13, error=0.1979, alpha=0.6997\n",
            "Pohon ke-9, max_depth=13, error=0.3282, alpha=0.3583\n",
            "Pohon ke-10, max_depth=13, error=0.2755, alpha=0.4835\n",
            "\n",
            "=== max_depth = 13 ===\n",
            "Akurasi: 0.7300 (73.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.51      0.53        59\n",
            "           1       0.80      0.82      0.81       141\n",
            "\n",
            "    accuracy                           0.73       200\n",
            "   macro avg       0.67      0.67      0.67       200\n",
            "weighted avg       0.72      0.73      0.73       200\n",
            "\n",
            "Pohon ke-1, max_depth=14, error=0.1813, alpha=0.7540\n",
            "Pohon ke-2, max_depth=14, error=0.1466, alpha=0.8806\n",
            "Pohon ke-3, max_depth=14, error=0.1869, alpha=0.7351\n",
            "Pohon ke-4, max_depth=14, error=0.1687, alpha=0.7974\n",
            "Pohon ke-5, max_depth=14, error=0.1606, alpha=0.8267\n",
            "Pohon ke-6, max_depth=14, error=0.2000, alpha=0.6932\n",
            "Pohon ke-7, max_depth=14, error=0.1711, alpha=0.7888\n",
            "Pohon ke-8, max_depth=14, error=0.1962, alpha=0.7050\n",
            "Pohon ke-9, max_depth=14, error=0.2214, alpha=0.6287\n",
            "Pohon ke-10, max_depth=14, error=0.1639, alpha=0.8148\n",
            "\n",
            "=== max_depth = 14 ===\n",
            "Akurasi: 0.7550 (75.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.58      0.58        59\n",
            "           1       0.82      0.83      0.83       141\n",
            "\n",
            "    accuracy                           0.76       200\n",
            "   macro avg       0.71      0.70      0.70       200\n",
            "weighted avg       0.75      0.76      0.75       200\n",
            "\n",
            "Pohon ke-1, max_depth=15, error=0.1825, alpha=0.7498\n",
            "Pohon ke-2, max_depth=15, error=0.1401, alpha=0.9074\n",
            "Pohon ke-3, max_depth=15, error=0.1956, alpha=0.7071\n",
            "Pohon ke-4, max_depth=15, error=0.1486, alpha=0.8726\n",
            "Pohon ke-5, max_depth=15, error=0.2325, alpha=0.5972\n",
            "Pohon ke-6, max_depth=15, error=0.1530, alpha=0.8556\n",
            "Pohon ke-7, max_depth=15, error=0.1715, alpha=0.7876\n",
            "Pohon ke-8, max_depth=15, error=0.1748, alpha=0.7759\n",
            "Pohon ke-9, max_depth=15, error=0.1864, alpha=0.7367\n",
            "Pohon ke-10, max_depth=15, error=0.1005, alpha=1.0956\n",
            "\n",
            "=== max_depth = 15 ===\n",
            "Akurasi: 0.7150 (71.50%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.42      0.47        59\n",
            "           1       0.78      0.84      0.81       141\n",
            "\n",
            "    accuracy                           0.71       200\n",
            "   macro avg       0.65      0.63      0.64       200\n",
            "weighted avg       0.70      0.71      0.71       200\n",
            "\n",
            "    max_depth  accuracy  execution_time\n",
            "0           1     0.715        0.634022\n",
            "1           2     0.745        1.561160\n",
            "2           3     0.760        3.277005\n",
            "3           4     0.750        4.017397\n",
            "4           5     0.780        5.870572\n",
            "5           6     0.700        7.524666\n",
            "6           7     0.740       11.830050\n",
            "7           8     0.715       14.353364\n",
            "8           9     0.730       16.026663\n",
            "9          10     0.685       18.516906\n",
            "10         11     0.710       22.964261\n",
            "11         12     0.705       29.381495\n",
            "12         13     0.730       27.351874\n",
            "13         14     0.755       30.790543\n",
            "14         15     0.715       32.891399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Menggunakan Adaboot dengan split Data 70:30 Gunakan Program Ini karena ada kedalaman Pohon**"
      ],
      "metadata": {
        "id": "H_Lb1j6NOh4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dan preprocessing data\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Fungsi entropy, gain_ratio_numeric, gain_ratio_categorical, best_split, build_tree, predict\n",
        "\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes, max_depth, current_depth=0):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0 or current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            # Jika nilai kategori tidak ada di cabang, pilih kelas mayoritas leaf terdekat\n",
        "            return 0\n",
        "\n",
        "def weighted_sample(data, weights):\n",
        "    n_samples = len(data)\n",
        "    indices = np.random.choice(n_samples, size=n_samples, replace=True, p=weights)\n",
        "    return data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "def adaBoost(train_data, test_data, attributes, target='class', n_estimators=10, max_depth=3):\n",
        "    n_samples = len(train_data)\n",
        "    weights = np.ones(n_samples) / n_samples\n",
        "    trees = []\n",
        "    alphas = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_data = weighted_sample(train_data, weights)\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        preds = np.array([predict(tree, row) for _, row in train_data.iterrows()])\n",
        "        actual = train_data[target].values\n",
        "        incorrect = (preds != actual).astype(int)\n",
        "        error = np.sum(weights * incorrect) / np.sum(weights)\n",
        "        if error > 0.5:\n",
        "            continue\n",
        "        if error == 0:\n",
        "            error = 1e-10\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        weights = weights * np.exp(-alpha * (1 - 2*incorrect))\n",
        "        weights = weights / np.sum(weights)\n",
        "        trees.append(tree)\n",
        "        alphas.append(alpha)\n",
        "        print(f\"Pohon ke-{m+1}, max_depth={max_depth}, error={error:.4f}, alpha={alpha:.4f}\")\n",
        "\n",
        "    def predict_ensemble(sample):\n",
        "        class_votes = {}\n",
        "        for tree, alpha in zip(trees, alphas):\n",
        "            pred = predict(tree, sample)\n",
        "            class_votes[pred] = class_votes.get(pred, 0) + alpha\n",
        "        return max(class_votes, key=class_votes.get)\n",
        "\n",
        "    predictions = [predict_ensemble(row) for _, row in test_data.iterrows()]\n",
        "    accuracy = accuracy_score(test_data[target], predictions)\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    return accuracy, exec_time, n_estimators, max_depth, predictions\n",
        "\n",
        "# Jalankan AdaBoost dan tampilkan laporan klasifikasi + akurasi per max_depth\n",
        "results = []\n",
        "for depth in range(1, 16):\n",
        "    acc, t_exec, n_trees, max_d, preds = adaBoost(train_data, test_data, attributes, target='class',\n",
        "                                                  n_estimators=10, max_depth=depth)\n",
        "    print(f\"\\n=== max_depth = {max_d} ===\")\n",
        "    print(f\"Akurasi: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "    print(\"Laporan Klasifikasi:\")\n",
        "    print(classification_report(test_data['class'], preds))\n",
        "    results.append((max_d, acc, t_exec))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['max_depth', 'accuracy', 'execution_time'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE1_OKx6OiFb",
        "outputId": "fd336f14-190a-414d-cc2b-1769eb15cc95"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pohon ke-1, max_depth=1, error=0.2986, alpha=0.4271\n",
            "Pohon ke-2, max_depth=1, error=0.3252, alpha=0.3649\n",
            "Pohon ke-3, max_depth=1, error=0.4543, alpha=0.0916\n",
            "Pohon ke-4, max_depth=1, error=0.4650, alpha=0.0701\n",
            "Pohon ke-5, max_depth=1, error=0.4396, alpha=0.1215\n",
            "Pohon ke-6, max_depth=1, error=0.4468, alpha=0.1069\n",
            "Pohon ke-7, max_depth=1, error=0.4289, alpha=0.1431\n",
            "Pohon ke-8, max_depth=1, error=0.4801, alpha=0.0398\n",
            "Pohon ke-9, max_depth=1, error=0.4720, alpha=0.0560\n",
            "Pohon ke-10, max_depth=1, error=0.4362, alpha=0.1284\n",
            "\n",
            "=== max_depth = 1 ===\n",
            "Akurasi: 0.7100 (71.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.15      0.24        91\n",
            "           1       0.72      0.95      0.82       209\n",
            "\n",
            "    accuracy                           0.71       300\n",
            "   macro avg       0.65      0.55      0.53       300\n",
            "weighted avg       0.68      0.71      0.65       300\n",
            "\n",
            "Pohon ke-1, max_depth=2, error=0.2686, alpha=0.5009\n",
            "Pohon ke-2, max_depth=2, error=0.3919, alpha=0.2197\n",
            "Pohon ke-3, max_depth=2, error=0.3881, alpha=0.2276\n",
            "Pohon ke-4, max_depth=2, error=0.4350, alpha=0.1307\n",
            "Pohon ke-5, max_depth=2, error=0.4106, alpha=0.1808\n",
            "Pohon ke-6, max_depth=2, error=0.4258, alpha=0.1495\n",
            "Pohon ke-7, max_depth=2, error=0.3861, alpha=0.2319\n",
            "Pohon ke-8, max_depth=2, error=0.4786, alpha=0.0428\n",
            "Pohon ke-9, max_depth=2, error=0.4788, alpha=0.0423\n",
            "Pohon ke-10, max_depth=2, error=0.4559, alpha=0.0884\n",
            "\n",
            "=== max_depth = 2 ===\n",
            "Akurasi: 0.7600 (76.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.36      0.48        91\n",
            "           1       0.77      0.93      0.84       209\n",
            "\n",
            "    accuracy                           0.76       300\n",
            "   macro avg       0.74      0.65      0.66       300\n",
            "weighted avg       0.75      0.76      0.73       300\n",
            "\n",
            "Pohon ke-1, max_depth=3, error=0.2771, alpha=0.4793\n",
            "Pohon ke-2, max_depth=3, error=0.3269, alpha=0.3611\n",
            "Pohon ke-3, max_depth=3, error=0.4132, alpha=0.1754\n",
            "Pohon ke-4, max_depth=3, error=0.3916, alpha=0.2203\n",
            "Pohon ke-5, max_depth=3, error=0.4322, alpha=0.1365\n",
            "Pohon ke-6, max_depth=3, error=0.4720, alpha=0.0560\n",
            "Pohon ke-7, max_depth=3, error=0.4626, alpha=0.0749\n",
            "Pohon ke-8, max_depth=3, error=0.4165, alpha=0.1686\n",
            "Pohon ke-9, max_depth=3, error=0.4362, alpha=0.1283\n",
            "Pohon ke-10, max_depth=3, error=0.4386, alpha=0.1235\n",
            "\n",
            "=== max_depth = 3 ===\n",
            "Akurasi: 0.7267 (72.67%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.32      0.41        91\n",
            "           1       0.75      0.90      0.82       209\n",
            "\n",
            "    accuracy                           0.73       300\n",
            "   macro avg       0.67      0.61      0.62       300\n",
            "weighted avg       0.70      0.73      0.70       300\n",
            "\n",
            "Pohon ke-1, max_depth=4, error=0.2714, alpha=0.4937\n",
            "Pohon ke-2, max_depth=4, error=0.3068, alpha=0.4077\n",
            "Pohon ke-3, max_depth=4, error=0.4015, alpha=0.1997\n",
            "Pohon ke-4, max_depth=4, error=0.4117, alpha=0.1784\n",
            "Pohon ke-5, max_depth=4, error=0.4058, alpha=0.1908\n",
            "Pohon ke-6, max_depth=4, error=0.4485, alpha=0.1034\n",
            "Pohon ke-7, max_depth=4, error=0.4216, alpha=0.1581\n",
            "Pohon ke-8, max_depth=4, error=0.4148, alpha=0.1720\n",
            "Pohon ke-9, max_depth=4, error=0.3721, alpha=0.2616\n",
            "Pohon ke-10, max_depth=4, error=0.4212, alpha=0.1589\n",
            "\n",
            "=== max_depth = 4 ===\n",
            "Akurasi: 0.7567 (75.67%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.37      0.48        91\n",
            "           1       0.77      0.92      0.84       209\n",
            "\n",
            "    accuracy                           0.76       300\n",
            "   macro avg       0.73      0.65      0.66       300\n",
            "weighted avg       0.74      0.76      0.73       300\n",
            "\n",
            "Pohon ke-1, max_depth=5, error=0.2629, alpha=0.5156\n",
            "Pohon ke-2, max_depth=5, error=0.2987, alpha=0.4268\n",
            "Pohon ke-3, max_depth=5, error=0.4052, alpha=0.1919\n",
            "Pohon ke-4, max_depth=5, error=0.3420, alpha=0.3271\n",
            "Pohon ke-5, max_depth=5, error=0.3579, alpha=0.2923\n",
            "Pohon ke-6, max_depth=5, error=0.3743, alpha=0.2569\n",
            "Pohon ke-7, max_depth=5, error=0.3692, alpha=0.2678\n",
            "Pohon ke-8, max_depth=5, error=0.4240, alpha=0.1532\n",
            "Pohon ke-9, max_depth=5, error=0.4157, alpha=0.1702\n",
            "Pohon ke-10, max_depth=5, error=0.4128, alpha=0.1763\n",
            "\n",
            "=== max_depth = 5 ===\n",
            "Akurasi: 0.7100 (71.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.37      0.44        91\n",
            "           1       0.76      0.86      0.80       209\n",
            "\n",
            "    accuracy                           0.71       300\n",
            "   macro avg       0.64      0.62      0.62       300\n",
            "weighted avg       0.69      0.71      0.69       300\n",
            "\n",
            "Pohon ke-1, max_depth=6, error=0.2543, alpha=0.5379\n",
            "Pohon ke-2, max_depth=6, error=0.3017, alpha=0.4196\n",
            "Pohon ke-3, max_depth=6, error=0.3390, alpha=0.3339\n",
            "Pohon ke-4, max_depth=6, error=0.3691, alpha=0.2680\n",
            "Pohon ke-5, max_depth=6, error=0.3621, alpha=0.2832\n",
            "Pohon ke-6, max_depth=6, error=0.4008, alpha=0.2011\n",
            "Pohon ke-7, max_depth=6, error=0.3852, alpha=0.2338\n",
            "Pohon ke-8, max_depth=6, error=0.3683, alpha=0.2697\n",
            "Pohon ke-9, max_depth=6, error=0.3822, alpha=0.2402\n",
            "Pohon ke-10, max_depth=6, error=0.3895, alpha=0.2246\n",
            "\n",
            "=== max_depth = 6 ===\n",
            "Akurasi: 0.7167 (71.67%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.41      0.47        91\n",
            "           1       0.77      0.85      0.81       209\n",
            "\n",
            "    accuracy                           0.72       300\n",
            "   macro avg       0.66      0.63      0.64       300\n",
            "weighted avg       0.70      0.72      0.70       300\n",
            "\n",
            "Pohon ke-1, max_depth=7, error=0.2186, alpha=0.6370\n",
            "Pohon ke-2, max_depth=7, error=0.2427, alpha=0.5690\n",
            "Pohon ke-3, max_depth=7, error=0.3599, alpha=0.2880\n",
            "Pohon ke-4, max_depth=7, error=0.3465, alpha=0.3172\n",
            "Pohon ke-5, max_depth=7, error=0.3312, alpha=0.3513\n",
            "Pohon ke-6, max_depth=7, error=0.3711, alpha=0.2637\n",
            "Pohon ke-7, max_depth=7, error=0.3739, alpha=0.2578\n",
            "Pohon ke-8, max_depth=7, error=0.3750, alpha=0.2555\n",
            "Pohon ke-9, max_depth=7, error=0.4302, alpha=0.1406\n",
            "Pohon ke-10, max_depth=7, error=0.3582, alpha=0.2916\n",
            "\n",
            "=== max_depth = 7 ===\n",
            "Akurasi: 0.6800 (68.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.53      0.50        91\n",
            "           1       0.78      0.75      0.76       209\n",
            "\n",
            "    accuracy                           0.68       300\n",
            "   macro avg       0.63      0.64      0.63       300\n",
            "weighted avg       0.69      0.68      0.68       300\n",
            "\n",
            "Pohon ke-1, max_depth=8, error=0.2357, alpha=0.5882\n",
            "Pohon ke-2, max_depth=8, error=0.2521, alpha=0.5436\n",
            "Pohon ke-3, max_depth=8, error=0.3045, alpha=0.4130\n",
            "Pohon ke-4, max_depth=8, error=0.2933, alpha=0.4397\n",
            "Pohon ke-5, max_depth=8, error=0.2980, alpha=0.4284\n",
            "Pohon ke-6, max_depth=8, error=0.3422, alpha=0.3267\n",
            "Pohon ke-7, max_depth=8, error=0.3876, alpha=0.2287\n",
            "Pohon ke-8, max_depth=8, error=0.3868, alpha=0.2303\n",
            "Pohon ke-9, max_depth=8, error=0.3610, alpha=0.2855\n",
            "Pohon ke-10, max_depth=8, error=0.3770, alpha=0.2512\n",
            "\n",
            "=== max_depth = 8 ===\n",
            "Akurasi: 0.6933 (69.33%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.29      0.36        91\n",
            "           1       0.74      0.87      0.80       209\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.61      0.58      0.58       300\n",
            "weighted avg       0.66      0.69      0.67       300\n",
            "\n",
            "Pohon ke-1, max_depth=9, error=0.2200, alpha=0.6328\n",
            "Pohon ke-2, max_depth=9, error=0.2717, alpha=0.4929\n",
            "Pohon ke-3, max_depth=9, error=0.2365, alpha=0.5859\n",
            "Pohon ke-4, max_depth=9, error=0.3086, alpha=0.4034\n",
            "Pohon ke-5, max_depth=9, error=0.2622, alpha=0.5172\n",
            "Pohon ke-6, max_depth=9, error=0.4220, alpha=0.1573\n",
            "Pohon ke-7, max_depth=9, error=0.3340, alpha=0.3450\n",
            "Pohon ke-8, max_depth=9, error=0.3244, alpha=0.3667\n",
            "Pohon ke-9, max_depth=9, error=0.2423, alpha=0.5700\n",
            "Pohon ke-10, max_depth=9, error=0.2745, alpha=0.4859\n",
            "\n",
            "=== max_depth = 9 ===\n",
            "Akurasi: 0.7033 (70.33%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.44      0.47        91\n",
            "           1       0.77      0.82      0.79       209\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.64      0.63      0.63       300\n",
            "weighted avg       0.69      0.70      0.70       300\n",
            "\n",
            "Pohon ke-1, max_depth=10, error=0.1986, alpha=0.6976\n",
            "Pohon ke-2, max_depth=10, error=0.1912, alpha=0.7212\n",
            "Pohon ke-3, max_depth=10, error=0.2443, alpha=0.5647\n",
            "Pohon ke-4, max_depth=10, error=0.2474, alpha=0.5564\n",
            "Pohon ke-5, max_depth=10, error=0.3006, alpha=0.4223\n",
            "Pohon ke-6, max_depth=10, error=0.3229, alpha=0.3702\n",
            "Pohon ke-7, max_depth=10, error=0.2705, alpha=0.4961\n",
            "Pohon ke-8, max_depth=10, error=0.2155, alpha=0.6460\n",
            "Pohon ke-9, max_depth=10, error=0.2476, alpha=0.5558\n",
            "Pohon ke-10, max_depth=10, error=0.2285, alpha=0.6085\n",
            "\n",
            "=== max_depth = 10 ===\n",
            "Akurasi: 0.7267 (72.67%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.49      0.52        91\n",
            "           1       0.79      0.83      0.81       209\n",
            "\n",
            "    accuracy                           0.73       300\n",
            "   macro avg       0.67      0.66      0.67       300\n",
            "weighted avg       0.72      0.73      0.72       300\n",
            "\n",
            "Pohon ke-1, max_depth=11, error=0.1843, alpha=0.7438\n",
            "Pohon ke-2, max_depth=11, error=0.2000, alpha=0.6931\n",
            "Pohon ke-3, max_depth=11, error=0.3074, alpha=0.4062\n",
            "Pohon ke-4, max_depth=11, error=0.2326, alpha=0.5968\n",
            "Pohon ke-5, max_depth=11, error=0.2783, alpha=0.4764\n",
            "Pohon ke-6, max_depth=11, error=0.2877, alpha=0.4532\n",
            "Pohon ke-7, max_depth=11, error=0.3007, alpha=0.4219\n",
            "Pohon ke-8, max_depth=11, error=0.2993, alpha=0.4253\n",
            "Pohon ke-9, max_depth=11, error=0.3158, alpha=0.3865\n",
            "Pohon ke-10, max_depth=11, error=0.3426, alpha=0.3258\n",
            "\n",
            "=== max_depth = 11 ===\n",
            "Akurasi: 0.7367 (73.67%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.45      0.51        91\n",
            "           1       0.78      0.86      0.82       209\n",
            "\n",
            "    accuracy                           0.74       300\n",
            "   macro avg       0.68      0.66      0.66       300\n",
            "weighted avg       0.72      0.74      0.73       300\n",
            "\n",
            "Pohon ke-1, max_depth=12, error=0.1829, alpha=0.7486\n",
            "Pohon ke-2, max_depth=12, error=0.1655, alpha=0.8091\n",
            "Pohon ke-3, max_depth=12, error=0.1953, alpha=0.7079\n",
            "Pohon ke-4, max_depth=12, error=0.1885, alpha=0.7300\n",
            "Pohon ke-5, max_depth=12, error=0.2218, alpha=0.6276\n",
            "Pohon ke-6, max_depth=12, error=0.2697, alpha=0.4982\n",
            "Pohon ke-7, max_depth=12, error=0.2700, alpha=0.4972\n",
            "Pohon ke-8, max_depth=12, error=0.2322, alpha=0.5980\n",
            "Pohon ke-9, max_depth=12, error=0.3819, alpha=0.2408\n",
            "Pohon ke-10, max_depth=12, error=0.1903, alpha=0.7241\n",
            "\n",
            "=== max_depth = 12 ===\n",
            "Akurasi: 0.7100 (71.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.45      0.49        91\n",
            "           1       0.77      0.82      0.80       209\n",
            "\n",
            "    accuracy                           0.71       300\n",
            "   macro avg       0.65      0.64      0.64       300\n",
            "weighted avg       0.70      0.71      0.70       300\n",
            "\n",
            "Pohon ke-1, max_depth=13, error=0.1857, alpha=0.7391\n",
            "Pohon ke-2, max_depth=13, error=0.2011, alpha=0.6896\n",
            "Pohon ke-3, max_depth=13, error=0.1631, alpha=0.8177\n",
            "Pohon ke-4, max_depth=13, error=0.1833, alpha=0.7469\n",
            "Pohon ke-5, max_depth=13, error=0.2528, alpha=0.5420\n",
            "Pohon ke-6, max_depth=13, error=0.2318, alpha=0.5990\n",
            "Pohon ke-7, max_depth=13, error=0.3014, alpha=0.4202\n",
            "Pohon ke-8, max_depth=13, error=0.2112, alpha=0.6589\n",
            "Pohon ke-9, max_depth=13, error=0.2862, alpha=0.4571\n",
            "Pohon ke-10, max_depth=13, error=0.2403, alpha=0.5754\n",
            "\n",
            "=== max_depth = 13 ===\n",
            "Akurasi: 0.6900 (69.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.42      0.45        91\n",
            "           1       0.76      0.81      0.78       209\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.62      0.61      0.62       300\n",
            "weighted avg       0.68      0.69      0.68       300\n",
            "\n",
            "Pohon ke-1, max_depth=14, error=0.1657, alpha=0.8082\n",
            "Pohon ke-2, max_depth=14, error=0.1760, alpha=0.7719\n",
            "Pohon ke-3, max_depth=14, error=0.1752, alpha=0.7747\n",
            "Pohon ke-4, max_depth=14, error=0.1865, alpha=0.7363\n",
            "Pohon ke-5, max_depth=14, error=0.2133, alpha=0.6526\n",
            "Pohon ke-6, max_depth=14, error=0.1202, alpha=0.9952\n",
            "Pohon ke-7, max_depth=14, error=0.1662, alpha=0.8063\n",
            "Pohon ke-8, max_depth=14, error=0.1615, alpha=0.8236\n",
            "Pohon ke-9, max_depth=14, error=0.1731, alpha=0.7819\n",
            "Pohon ke-10, max_depth=14, error=0.1433, alpha=0.8941\n",
            "\n",
            "=== max_depth = 14 ===\n",
            "Akurasi: 0.6900 (69.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.36      0.42        91\n",
            "           1       0.75      0.83      0.79       209\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.62      0.60      0.60       300\n",
            "weighted avg       0.67      0.69      0.68       300\n",
            "\n",
            "Pohon ke-1, max_depth=15, error=0.1771, alpha=0.7679\n",
            "Pohon ke-2, max_depth=15, error=0.1650, alpha=0.8107\n",
            "Pohon ke-3, max_depth=15, error=0.1574, alpha=0.8389\n",
            "Pohon ke-4, max_depth=15, error=0.1775, alpha=0.7668\n",
            "Pohon ke-5, max_depth=15, error=0.3172, alpha=0.3834\n",
            "Pohon ke-6, max_depth=15, error=0.1820, alpha=0.7514\n",
            "Pohon ke-7, max_depth=15, error=0.2239, alpha=0.6216\n",
            "Pohon ke-8, max_depth=15, error=0.1708, alpha=0.7901\n",
            "Pohon ke-9, max_depth=15, error=0.2113, alpha=0.6586\n",
            "Pohon ke-10, max_depth=15, error=0.1111, alpha=1.0398\n",
            "\n",
            "=== max_depth = 15 ===\n",
            "Akurasi: 0.7300 (73.00%)\n",
            "Laporan Klasifikasi:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.51      0.53        91\n",
            "           1       0.79      0.83      0.81       209\n",
            "\n",
            "    accuracy                           0.73       300\n",
            "   macro avg       0.68      0.67      0.67       300\n",
            "weighted avg       0.72      0.73      0.73       300\n",
            "\n",
            "    max_depth  accuracy  execution_time\n",
            "0           1  0.710000        0.605597\n",
            "1           2  0.760000        1.286954\n",
            "2           3  0.726667        2.905778\n",
            "3           4  0.756667        4.396388\n",
            "4           5  0.710000        6.044713\n",
            "5           6  0.716667        8.742020\n",
            "6           7  0.680000       10.531327\n",
            "7           8  0.693333       11.963682\n",
            "8           9  0.703333       15.599437\n",
            "9          10  0.726667       18.813587\n",
            "10         11  0.736667       18.418612\n",
            "11         12  0.710000       22.009928\n",
            "12         13  0.690000       23.477898\n",
            "13         14  0.690000       26.139192\n",
            "14         15  0.730000       26.676441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jangan Gunakan Program Ini**"
      ],
      "metadata": {
        "id": "Eza5bX7LMXBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dan preprocessing data\n",
        "data = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "data = data[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "             'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        try:\n",
        "            X[col] = X[col].astype(float)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "data_cleaned = X.copy()\n",
        "data_cleaned['class'] = y.copy()\n",
        "data_cleaned.fillna(data_cleaned.median(numeric_only=True), inplace=True)\n",
        "for col in data_cleaned.select_dtypes(include='object'):\n",
        "    data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "y_encoded = LabelEncoder().fit_transform(data_cleaned['class'])\n",
        "data_cleaned['class'] = y_encoded\n",
        "\n",
        "X = data_cleaned.drop(columns=['class'])\n",
        "y = data_cleaned['class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = X_train.copy()\n",
        "train_data['class'] = y_train\n",
        "test_data = X_test.copy()\n",
        "test_data['class'] = y_test\n",
        "\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Definisi fungsi entropy, gain_ratio_numeric, gain_ratio_categorical, best_split, build_tree, predict\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio_numeric(data, attr, target, base_entropy):\n",
        "    values = np.sort(data[attr].unique())\n",
        "    if len(values) <= 1:\n",
        "        return 0, None\n",
        "    percentiles = np.percentile(values, [10, 30, 50, 70, 90])\n",
        "    best_gain = 0\n",
        "    best_threshold = None\n",
        "    for threshold in percentiles:\n",
        "        left = data[data[attr] <= threshold]\n",
        "        right = data[data[attr] > threshold]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "        left_entropy = entropy(left, target)\n",
        "        right_entropy = entropy(right, target)\n",
        "        weighted_entropy = (len(left) * left_entropy + len(right) * right_entropy) / len(data)\n",
        "        info_gain = base_entropy - weighted_entropy\n",
        "        split_info = -((len(left) / len(data)) * np.log2(len(left) / len(data) + 1e-9) +\n",
        "                       (len(right) / len(data)) * np.log2(len(right) / len(data) + 1e-9))\n",
        "        if split_info == 0:\n",
        "            continue\n",
        "        gain_ratio = info_gain / split_info\n",
        "        if gain_ratio > best_gain:\n",
        "            best_gain = gain_ratio\n",
        "            best_threshold = threshold\n",
        "    return best_gain, best_threshold\n",
        "\n",
        "def gain_ratio_categorical(data, attr, target, base_entropy):\n",
        "    values = data[attr].unique()\n",
        "    weighted_entropy = 0\n",
        "    split_info = 0\n",
        "    for val in values:\n",
        "        subset = data[data[attr] == val]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        subset_entropy = entropy(subset, target)\n",
        "        weight = len(subset) / len(data)\n",
        "        weighted_entropy += weight * subset_entropy\n",
        "        split_info -= weight * np.log2(weight + 1e-9)\n",
        "    info_gain = base_entropy - weighted_entropy\n",
        "    if split_info == 0:\n",
        "        return 0\n",
        "    return info_gain / split_info\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    base_entropy = entropy(data, target)\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = 0\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            gain, threshold = gain_ratio_numeric(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = threshold\n",
        "                is_numeric = True\n",
        "        else:\n",
        "            gain = gain_ratio_categorical(data, attr, target, base_entropy)\n",
        "            if gain > best_gain_ratio:\n",
        "                best_gain_ratio = gain\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "def build_tree(data, target, attributes, max_depth, current_depth=0):\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0 or current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def predict(tree, sample):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if sample[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], sample)\n",
        "        else:\n",
        "            return predict(tree['right'], sample)\n",
        "    else:\n",
        "        val = sample[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], sample)\n",
        "        else:\n",
        "            # Jika cabang tidak ada, kembalikan kelas mayoritas\n",
        "            return 0\n",
        "\n",
        "def weighted_sample(data, weights):\n",
        "    n_samples = len(data)\n",
        "    indices = np.random.choice(n_samples, size=n_samples, replace=True, p=weights)\n",
        "    return data.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "def adaBoost(train_data, test_data, attributes, target='class', n_estimators=10, max_depth=3):\n",
        "    n_samples = len(train_data)\n",
        "    weights = np.ones(n_samples) / n_samples\n",
        "    trees = []\n",
        "    alphas = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_data = weighted_sample(train_data, weights)\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        preds = np.array([predict(tree, row) for _, row in train_data.iterrows()])\n",
        "        actual = train_data[target].values\n",
        "        incorrect = (preds != actual).astype(int)\n",
        "        error = np.sum(weights * incorrect) / np.sum(weights)\n",
        "        if error > 0.5:\n",
        "            continue\n",
        "        if error == 0:\n",
        "            error = 1e-10\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        weights = weights * np.exp(-alpha * (1 - 2*incorrect))\n",
        "        weights = weights / np.sum(weights)\n",
        "        trees.append(tree)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "    def predict_ensemble(sample):\n",
        "        class_votes = {}\n",
        "        for tree, alpha in zip(trees, alphas):\n",
        "            pred = predict(tree, sample)\n",
        "            class_votes[pred] = class_votes.get(pred, 0) + alpha\n",
        "        return max(class_votes, key=class_votes.get)\n",
        "\n",
        "    predictions = [predict_ensemble(row) for _, row in test_data.iterrows()]\n",
        "    accuracy = accuracy_score(test_data[target], predictions)\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    # Cetak laporan klasifikasi\n",
        "    print(\"Classification Report:\\n\")\n",
        "    print(classification_report(test_data[target], predictions))\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Execution time: {exec_time:.2f} seconds\\n\")\n",
        "\n",
        "    return accuracy, exec_time\n",
        "\n",
        "# Jalankan AdaBoost sekali dengan max_depth yang diinginkan, misal 3\n",
        "accuracy, exec_time = adaBoost(train_data, test_data, attributes, target='class', n_estimators=10, max_depth=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZQAFLC5MXKM",
        "outputId": "bf5e573b-27dd-411d-c444-664a5fe7b294"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.15      0.25        59\n",
            "           1       0.73      0.97      0.84       141\n",
            "\n",
            "    accuracy                           0.73       200\n",
            "   macro avg       0.71      0.56      0.54       200\n",
            "weighted avg       0.72      0.73      0.66       200\n",
            "\n",
            "Accuracy: 0.7300\n",
            "Execution time: 2.39 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Traning 80:20 Dengan AdaBoots**"
      ],
      "metadata": {
        "id": "7PzLcS1SFjxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi Gain Ratio & Split ===\n",
        "# ===============================\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def info_gain(data, attr, target):\n",
        "    total_entropy = entropy(data, target)\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) *\n",
        "                              entropy(data[data[attr] == values[i]], target)\n",
        "                              for i in range(len(values))])\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "def split_info(data, attr):\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def gain_ratio(data, attr, target):\n",
        "    si = split_info(data, attr)\n",
        "    if si == 0:\n",
        "        return 0\n",
        "    return info_gain(data, attr, target) / si\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    best_attr = None\n",
        "    best_gain_ratio = -1\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            thresholds = data[attr].sort_values().unique()\n",
        "            for t in thresholds[:-1]:\n",
        "                data_copy = data.copy()\n",
        "                data_copy[attr] = data_copy[attr] <= t\n",
        "                gr = gain_ratio(data_copy, attr, target)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = t\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "# ==========================\n",
        "# === Pembangunan Tree ===\n",
        "# ==========================\n",
        "def build_tree(data, target, attributes, max_depth=None, current_depth=0):\n",
        "    if len(data) == 0:\n",
        "        return None  # Data kosong, leaf kosong\n",
        "\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    if max_depth is not None and current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def get_tree_depth(tree):\n",
        "    if tree is None:\n",
        "        return 0\n",
        "    if tree['type'] == 'leaf':\n",
        "        return 0\n",
        "    if tree['is_numeric']:\n",
        "        return 1 + max(get_tree_depth(tree['left']), get_tree_depth(tree['right']))\n",
        "    else:\n",
        "        return 1 + max(get_tree_depth(branch) for branch in tree['branches'].values())\n",
        "\n",
        "# =======================\n",
        "# === Prediksi & Boost ===\n",
        "# =======================\n",
        "def predict(tree, row):\n",
        "    if tree is None:\n",
        "        return 0  # default jika tree kosong\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if row[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], row)\n",
        "        else:\n",
        "            return predict(tree['right'], row)\n",
        "    else:\n",
        "        val = row[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], row)\n",
        "        else:\n",
        "            # default ke cabang pertama jika val tidak ada\n",
        "            return list(tree['branches'].values())[0]['class']\n",
        "\n",
        "def convert_label(y):\n",
        "    return np.array([1 if label == 1 else -1 for label in y])\n",
        "\n",
        "def ada_boost_train(data, target, attributes, n_estimators=10, max_depth=None):\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n\n",
        "    trees = []\n",
        "    alphas = []\n",
        "    depths = []\n",
        "    y_true = convert_label(data[target].values)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_indices = np.random.choice(n, size=n, replace=True, p=weights)\n",
        "        sample_data = data.iloc[sample_indices].reset_index(drop=True)\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        trees.append(tree)\n",
        "\n",
        "        depth = get_tree_depth(tree)\n",
        "        depths.append(depth)\n",
        "\n",
        "        preds = np.array([1 if predict(tree, row) == 1 else -1 for _, row in data.iterrows()])\n",
        "        miss = (preds != y_true).astype(int)\n",
        "        error = np.sum(weights * miss)\n",
        "\n",
        "        if error == 0:\n",
        "            alphas.append(1)\n",
        "            break\n",
        "        if error >= 0.5:\n",
        "            break\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        alphas.append(alpha)\n",
        "        weights *= np.exp(-alpha * y_true * preds)\n",
        "        weights /= np.sum(weights)\n",
        "    return trees, alphas, depths\n",
        "\n",
        "def ada_boost_predict(trees, alphas, row):\n",
        "    weighted_votes = sum(alpha * (1 if predict(tree, row) == 1 else -1)\n",
        "                         for tree, alpha in zip(trees, alphas))\n",
        "    return 1 if weighted_votes > 0 else 0\n",
        "\n",
        "# ====================\n",
        "# === Load Dataset ===\n",
        "# ====================\n",
        "df = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "\n",
        "# Pilih kolom relevan\n",
        "data = df[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "           'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "# Encoding kolom bertipe objek\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Gabungkan kembali fitur dan target ke satu dataframe agar fungsi bisa jalan\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# ================================\n",
        "# === Pelatihan & Evaluasi ===\n",
        "# ================================\n",
        "start = time.time()\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Set kedalaman maksimum pohon di sini\n",
        "max_tree_depth = 5\n",
        "\n",
        "trees, alphas, depths = ada_boost_train(train_data, 'class', attributes,\n",
        "                                        n_estimators=10, max_depth=max_tree_depth)\n",
        "end = time.time()\n",
        "\n",
        "y_pred = [ada_boost_predict(trees, alphas, row) for _, row in test_data.iterrows()]\n",
        "y_true = test_data['class'].values\n",
        "\n",
        "print(f\"\\nAkurasi: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(f\"Waktu eksekusi: {end - start:.2f} detik\")\n",
        "\n",
        "# ==============================\n",
        "# === Tampilkan Kedalaman ===\n",
        "# ==============================\n",
        "print(\"\\nKedalaman masing-masing pohon:\")\n",
        "for i, d in enumerate(depths, start=1):\n",
        "    print(f\"Pohon ke-{i}: kedalaman = {d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxHCscKFj56",
        "outputId": "cb4d8970-4b79-4769-e2f8-2eb5880560b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi: 0.7400\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.12      0.21        59\n",
            "           1       0.73      1.00      0.84       141\n",
            "\n",
            "    accuracy                           0.74       200\n",
            "   macro avg       0.87      0.56      0.53       200\n",
            "weighted avg       0.81      0.74      0.66       200\n",
            "\n",
            "Waktu eksekusi: 44.20 detik\n",
            "\n",
            "Kedalaman masing-masing pohon:\n",
            "Pohon ke-1: kedalaman = 5\n",
            "Pohon ke-2: kedalaman = 5\n",
            "Pohon ke-3: kedalaman = 5\n",
            "Pohon ke-4: kedalaman = 5\n",
            "Pohon ke-5: kedalaman = 5\n",
            "Pohon ke-6: kedalaman = 5\n",
            "Pohon ke-7: kedalaman = 5\n",
            "Pohon ke-8: kedalaman = 5\n",
            "Pohon ke-9: kedalaman = 5\n",
            "Pohon ke-10: kedalaman = 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iGRNhdTSLO32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi Gain Ratio & Split ===\n",
        "# ===============================\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def info_gain(data, attr, target):\n",
        "    total_entropy = entropy(data, target)\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) *\n",
        "                              entropy(data[data[attr] == values[i]], target)\n",
        "                              for i in range(len(values))])\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "def split_info(data, attr):\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "def gain_ratio(data, attr, target):\n",
        "    si = split_info(data, attr)\n",
        "    if si == 0:\n",
        "        return 0\n",
        "    return info_gain(data, attr, target) / si\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    best_attr = None\n",
        "    best_gain_ratio = -1\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            thresholds = data[attr].sort_values().unique()\n",
        "            for t in thresholds[:-1]:\n",
        "                data_copy = data.copy()\n",
        "                data_copy[attr] = data_copy[attr] <= t\n",
        "                gr = gain_ratio(data_copy, attr, target)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = t\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "# ==========================\n",
        "# === Pembangunan Tree ===\n",
        "# ==========================\n",
        "def build_tree(data, target, attributes, max_depth=None, current_depth=0):\n",
        "    if len(data) == 0:\n",
        "        return None  # Data kosong, leaf kosong\n",
        "\n",
        "    labels = data[target].unique()\n",
        "    if len(labels) == 1 or len(data) == 0:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "    if max_depth is not None and current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def get_tree_depth(tree):\n",
        "    if tree is None:\n",
        "        return 0\n",
        "    if tree['type'] == 'leaf':\n",
        "        return 0\n",
        "    if tree['is_numeric']:\n",
        "        return 1 + max(get_tree_depth(tree['left']), get_tree_depth(tree['right']))\n",
        "    else:\n",
        "        return 1 + max(get_tree_depth(branch) for branch in tree['branches'].values())\n",
        "\n",
        "# =======================\n",
        "# === Prediksi & Boost ===\n",
        "# =======================\n",
        "def predict(tree, row):\n",
        "    if tree is None:\n",
        "        return 0  # default jika tree kosong\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if row[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], row)\n",
        "        else:\n",
        "            return predict(tree['right'], row)\n",
        "    else:\n",
        "        val = row[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], row)\n",
        "        else:\n",
        "            # default ke cabang pertama jika val tidak ada\n",
        "            return list(tree['branches'].values())[0]['class']\n",
        "\n",
        "def convert_label(y):\n",
        "    return np.array([1 if label == 1 else -1 for label in y])\n",
        "\n",
        "def ada_boost_train(data, target, attributes, n_estimators=10, max_depth=None):\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n\n",
        "    trees = []\n",
        "    alphas = []\n",
        "    depths = []\n",
        "    y_true = convert_label(data[target].values)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_indices = np.random.choice(n, size=n, replace=True, p=weights)\n",
        "        sample_data = data.iloc[sample_indices].reset_index(drop=True)\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        trees.append(tree)\n",
        "\n",
        "        depth = get_tree_depth(tree)\n",
        "        depths.append(depth)\n",
        "\n",
        "        preds = np.array([1 if predict(tree, row) == 1 else -1 for _, row in data.iterrows()])\n",
        "        miss = (preds != y_true).astype(int)\n",
        "        error = np.sum(weights * miss)\n",
        "\n",
        "        if error == 0:\n",
        "            alphas.append(1)\n",
        "            break\n",
        "        if error >= 0.5:\n",
        "            break\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        alphas.append(alpha)\n",
        "        weights *= np.exp(-alpha * y_true * preds)\n",
        "        weights /= np.sum(weights)\n",
        "    return trees, alphas, depths\n",
        "\n",
        "def ada_boost_predict(trees, alphas, row):\n",
        "    weighted_votes = sum(alpha * (1 if predict(tree, row) == 1 else -1)\n",
        "                         for tree, alpha in zip(trees, alphas))\n",
        "    return 1 if weighted_votes > 0 else 0\n",
        "\n",
        "# ====================\n",
        "# === Load Dataset ===\n",
        "# ====================\n",
        "df = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "\n",
        "# Pilih kolom relevan\n",
        "data = df[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "           'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "# Encoding kolom bertipe objek\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Gabungkan kembali fitur dan target ke satu dataframe agar fungsi bisa jalan\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "# ================================\n",
        "# === Pelatihan & Evaluasi ===\n",
        "# ================================\n",
        "start = time.time()\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "# Set kedalaman maksimum pohon di sini\n",
        "max_tree_depth = 5\n",
        "\n",
        "trees, alphas, depths = ada_boost_train(train_data, 'class', attributes,\n",
        "                                        n_estimators=10, max_depth=max_tree_depth)\n",
        "end = time.time()\n",
        "\n",
        "y_pred = [ada_boost_predict(trees, alphas, row) for _, row in test_data.iterrows()]\n",
        "y_true = test_data['class'].values\n",
        "\n",
        "print(f\"\\nAkurasi: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(f\"Waktu eksekusi: {end - start:.2f} detik\")\n",
        "\n",
        "# ==============================\n",
        "# === Tampilkan Kedalaman ===\n",
        "# ==============================\n",
        "print(\"\\nKedalaman masing-masing pohon:\")\n",
        "for i, d in enumerate(depths, start=1):\n",
        "    print(f\"Pohon ke-{i}: kedalaman = {d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOfyAT4KLO_s",
        "outputId": "31fea12c-faa0-4b77-ee9f-f53853566a8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi: 0.7000\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.02      0.04        91\n",
            "           1       0.70      1.00      0.82       209\n",
            "\n",
            "    accuracy                           0.70       300\n",
            "   macro avg       0.68      0.51      0.43       300\n",
            "weighted avg       0.69      0.70      0.59       300\n",
            "\n",
            "Waktu eksekusi: 12.85 detik\n",
            "\n",
            "Kedalaman masing-masing pohon:\n",
            "Pohon ke-1: kedalaman = 5\n",
            "Pohon ke-2: kedalaman = 5\n",
            "Pohon ke-3: kedalaman = 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FupUU1flJWgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi Entropy, Gain Ratio ===\n",
        "# ===============================\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))  # +epsilon agar tidak log(0)\n",
        "\n",
        "def info_gain(data, attr, target):\n",
        "    total_entropy = entropy(data, target)\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    weighted_entropy = 0\n",
        "    for i, val in enumerate(values):\n",
        "        subset = data[data[attr] == val]\n",
        "        weighted_entropy += (counts[i] / counts.sum()) * entropy(subset, target)\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "def split_info(data, attr):\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio(data, attr, target):\n",
        "    si = split_info(data, attr)\n",
        "    if si == 0:\n",
        "        return 0\n",
        "    return info_gain(data, attr, target) / si\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi untuk cari split terbaik ===\n",
        "# ===============================\n",
        "def best_split(data, attributes, target):\n",
        "    best_attr = None\n",
        "    best_threshold = None\n",
        "    best_gain_ratio = -1\n",
        "    is_numeric = False\n",
        "\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            # Coba split dengan threshold untuk atribut numerik\n",
        "            values = np.sort(data[attr].unique())\n",
        "            for t in values[:-1]:\n",
        "                data_copy = data.copy()\n",
        "                # Ubah atribut jadi boolean <= threshold\n",
        "                data_copy[attr] = data_copy[attr] <= t\n",
        "                gr = gain_ratio(data_copy, attr, target)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = t\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            # Atribut kategorikal\n",
        "            gr = gain_ratio(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                best_threshold = None\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi untuk membangun pohon ===\n",
        "# ===============================\n",
        "def build_tree(data, target, attributes, max_depth=None, current_depth=0):\n",
        "    if len(data) == 0:\n",
        "        return None\n",
        "\n",
        "    # Jika semua target sama atau kedalaman max tercapai, buat leaf\n",
        "    unique_targets = data[target].unique()\n",
        "    if len(unique_targets) == 1 or (max_depth is not None and current_depth >= max_depth):\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type':'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type':'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    node = {'type':'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            subset = data[data[attr] == val]\n",
        "            node['branches'][val] = build_tree(subset, target, attributes, max_depth, current_depth + 1)\n",
        "\n",
        "    return node\n",
        "\n",
        "def get_tree_depth(tree):\n",
        "    if tree is None:\n",
        "        return 0\n",
        "    if tree['type'] == 'leaf':\n",
        "        return 0\n",
        "    if tree['is_numeric']:\n",
        "        return 1 + max(get_tree_depth(tree['left']), get_tree_depth(tree['right']))\n",
        "    else:\n",
        "        return 1 + max(get_tree_depth(branch) for branch in tree['branches'].values())\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi prediksi dari pohon ===\n",
        "# ===============================\n",
        "def predict(tree, row):\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if row[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], row)\n",
        "        else:\n",
        "            return predict(tree['right'], row)\n",
        "    else:\n",
        "        val = row[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], row)\n",
        "        else:\n",
        "            # fallback jika cabang tidak ada\n",
        "            return list(tree['branches'].values())[0]['class']\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi bantu untuk AdaBoost ===\n",
        "# ===============================\n",
        "def convert_label(y):\n",
        "    # Ubah label ke -1 dan 1 (kelas positif = 1)\n",
        "    return np.array([1 if label == 1 else -1 for label in y])\n",
        "\n",
        "def ada_boost_train(data, target, attributes, n_estimators=10, max_depth=None):\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n\n",
        "    trees = []\n",
        "    alphas = []\n",
        "    depths = []\n",
        "    y_true = convert_label(data[target].values)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        # Sampling data dengan probabilitas bobot\n",
        "        sample_indices = np.random.choice(n, size=n, replace=True, p=weights)\n",
        "        sample_data = data.iloc[sample_indices].reset_index(drop=True)\n",
        "\n",
        "        # Bangun pohon dengan data sampling\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        trees.append(tree)\n",
        "\n",
        "        # Simpan kedalaman pohon\n",
        "        depths.append(get_tree_depth(tree))\n",
        "\n",
        "        # Prediksi seluruh data training dengan pohon baru\n",
        "        preds = np.array([1 if predict(tree, row) == 1 else -1 for _, row in data.iterrows()])\n",
        "        miss = (preds != y_true).astype(int)\n",
        "        error = np.sum(weights * miss)\n",
        "\n",
        "        # Jika error 0 atau >= 0.5, hentikan\n",
        "        if error == 0:\n",
        "            alphas.append(1)\n",
        "            break\n",
        "        if error >= 0.5:\n",
        "            break\n",
        "\n",
        "        # Hitung alpha (berat pohon)\n",
        "        alpha = 0.5 * np.log((1 - error) / error)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "        # Update bobot data\n",
        "        weights *= np.exp(-alpha * y_true * preds)\n",
        "        weights /= np.sum(weights)\n",
        "\n",
        "    return trees, alphas, depths\n",
        "\n",
        "def ada_boost_predict(trees, alphas, row):\n",
        "    vote = 0\n",
        "    for tree, alpha in zip(trees, alphas):\n",
        "        pred = 1 if predict(tree, row) == 1 else -1\n",
        "        vote += alpha * pred\n",
        "    return 1 if vote > 0 else 0\n",
        "\n",
        "# ===============================\n",
        "# === Load dan siapkan dataset ===\n",
        "# ===============================\n",
        "df = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "\n",
        "# Pilih kolom yang relevan (ubah sesuai dataset kamu)\n",
        "data = df[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "           'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "# Encode kolom bertipe objek\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Pisah fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Gabungkan lagi untuk kemudahan proses\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Bagi data train-test\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# ===============================\n",
        "# === Pelatihan AdaBoost ===\n",
        "# ===============================\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "max_depth = 5  # Atur kedalaman pohon maksimal\n",
        "n_estimators = 10  # Jumlah weak learner\n",
        "\n",
        "start = time.time()\n",
        "trees, alphas, depths = ada_boost_train(train_data, 'class', attributes,\n",
        "                                        n_estimators=n_estimators, max_depth=max_depth)\n",
        "end = time.time()\n",
        "\n",
        "# ===============================\n",
        "# === Prediksi data test ===\n",
        "# ===============================\n",
        "y_pred = [ada_boost_predict(trees, alphas, row) for _, row in test_data.iterrows()]\n",
        "y_true = test_data['class'].values\n",
        "\n",
        "# ===============================\n",
        "# === Tampilkan hasil ===\n",
        "# ===============================\n",
        "print(f\"Akurasi: {accuracy_score(y_true, y_pred):.4f}\\n\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(f\"Waktu eksekusi: {end - start:.2f} detik\\n\")\n",
        "\n",
        "print(\"Kedalaman pohon tiap estimator:\")\n",
        "for i, d in enumerate(depths, 1):\n",
        "    print(f\"Pohon ke-{i}: kedalaman = {d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM33a1P-JWn1",
        "outputId": "b2647d6b-c6e8-4097-ecae-1af7032de995"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.7150\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.03      0.07        59\n",
            "           1       0.71      1.00      0.83       141\n",
            "\n",
            "    accuracy                           0.71       200\n",
            "   macro avg       0.86      0.52      0.45       200\n",
            "weighted avg       0.80      0.71      0.61       200\n",
            "\n",
            "Waktu eksekusi: 45.03 detik\n",
            "\n",
            "Kedalaman pohon tiap estimator:\n",
            "Pohon ke-1: kedalaman = 5\n",
            "Pohon ke-2: kedalaman = 5\n",
            "Pohon ke-3: kedalaman = 5\n",
            "Pohon ke-4: kedalaman = 5\n",
            "Pohon ke-5: kedalaman = 5\n",
            "Pohon ke-6: kedalaman = 5\n",
            "Pohon ke-7: kedalaman = 5\n",
            "Pohon ke-8: kedalaman = 5\n",
            "Pohon ke-9: kedalaman = 5\n",
            "Pohon ke-10: kedalaman = 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mu-93QYQK7rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# ===============================\n",
        "# === Fungsi Gain Ratio & Split ===\n",
        "# ===============================\n",
        "def entropy(data, target):\n",
        "    values, counts = np.unique(data[target], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))  # +1e-9 supaya tidak log(0)\n",
        "\n",
        "def info_gain(data, attr, target):\n",
        "    total_entropy = entropy(data, target)\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) *\n",
        "                              entropy(data[data[attr] == values[i]], target)\n",
        "                              for i in range(len(values))])\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "def split_info(data, attr):\n",
        "    values, counts = np.unique(data[attr], return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-9))\n",
        "\n",
        "def gain_ratio(data, attr, target):\n",
        "    si = split_info(data, attr)\n",
        "    if si == 0:\n",
        "        return 0\n",
        "    return info_gain(data, attr, target) / si\n",
        "\n",
        "def best_split(data, attributes, target):\n",
        "    best_attr = None\n",
        "    best_gain_ratio = -1\n",
        "    best_threshold = None\n",
        "    is_numeric = False\n",
        "    for attr in attributes:\n",
        "        if data[attr].dtype in [np.int64, np.float64]:\n",
        "            thresholds = np.unique(data[attr].sort_values())\n",
        "            for t in thresholds[:-1]:\n",
        "                data_copy = data.copy()\n",
        "                data_copy[attr] = data_copy[attr] <= t\n",
        "                gr = gain_ratio(data_copy, attr, target)\n",
        "                if gr > best_gain_ratio:\n",
        "                    best_gain_ratio = gr\n",
        "                    best_attr = attr\n",
        "                    best_threshold = t\n",
        "                    is_numeric = True\n",
        "        else:\n",
        "            gr = gain_ratio(data, attr, target)\n",
        "            if gr > best_gain_ratio:\n",
        "                best_gain_ratio = gr\n",
        "                best_attr = attr\n",
        "                is_numeric = False\n",
        "    return best_attr, best_threshold, is_numeric\n",
        "\n",
        "# ==========================\n",
        "# === Pembangunan Tree (Modifikasi) ===\n",
        "# ==========================\n",
        "def build_tree(data, target, attributes, max_depth=None, current_depth=0):\n",
        "    if len(data) == 0:\n",
        "        return None  # Data kosong, leaf kosong\n",
        "\n",
        "    # Jika max_depth tercapai, buat leaf dengan kelas mayoritas\n",
        "    if max_depth is not None and current_depth >= max_depth:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    attr, threshold, is_numeric = best_split(data, attributes, target)\n",
        "    # Jika tidak ada split yang bisa dilakukan, buat leaf\n",
        "    if attr is None:\n",
        "        values, counts = np.unique(data[target], return_counts=True)\n",
        "        return {'type': 'leaf', 'class': values[np.argmax(counts)]}\n",
        "\n",
        "    node = {'type': 'node', 'attribute': attr, 'is_numeric': is_numeric}\n",
        "\n",
        "    if is_numeric:\n",
        "        node['threshold'] = threshold\n",
        "        node['left'] = build_tree(data[data[attr] <= threshold], target, attributes, max_depth, current_depth + 1)\n",
        "        node['right'] = build_tree(data[data[attr] > threshold], target, attributes, max_depth, current_depth + 1)\n",
        "    else:\n",
        "        node['branches'] = {}\n",
        "        for val in data[attr].unique():\n",
        "            node['branches'][val] = build_tree(data[data[attr] == val], target, attributes, max_depth, current_depth + 1)\n",
        "    return node\n",
        "\n",
        "def get_tree_depth(tree):\n",
        "    if tree is None:\n",
        "        return 0\n",
        "    if tree['type'] == 'leaf':\n",
        "        return 0\n",
        "    if tree['is_numeric']:\n",
        "        return 1 + max(get_tree_depth(tree['left']), get_tree_depth(tree['right']))\n",
        "    else:\n",
        "        return 1 + max(get_tree_depth(branch) for branch in tree['branches'].values())\n",
        "\n",
        "# =======================\n",
        "# === Prediksi & Boost ===\n",
        "# =======================\n",
        "def predict(tree, row):\n",
        "    if tree is None:\n",
        "        return 0  # default jika tree kosong\n",
        "    if tree['type'] == 'leaf':\n",
        "        return tree['class']\n",
        "    attr = tree['attribute']\n",
        "    if tree['is_numeric']:\n",
        "        if row[attr] <= tree['threshold']:\n",
        "            return predict(tree['left'], row)\n",
        "        else:\n",
        "            return predict(tree['right'], row)\n",
        "    else:\n",
        "        val = row[attr]\n",
        "        if val in tree['branches']:\n",
        "            return predict(tree['branches'][val], row)\n",
        "        else:\n",
        "            # default ke cabang pertama jika val tidak ada\n",
        "            return list(tree['branches'].values())[0]['class']\n",
        "\n",
        "def convert_label(y):\n",
        "    return np.array([1 if label == 1 else -1 for label in y])\n",
        "\n",
        "def ada_boost_train(data, target, attributes, n_estimators=10, max_depth=None):\n",
        "    n = len(data)\n",
        "    weights = np.ones(n) / n\n",
        "    trees = []\n",
        "    alphas = []\n",
        "    depths = []\n",
        "    y_true = convert_label(data[target].values)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        sample_indices = np.random.choice(n, size=n, replace=True, p=weights)\n",
        "        sample_data = data.iloc[sample_indices].reset_index(drop=True)\n",
        "        tree = build_tree(sample_data, target, attributes, max_depth=max_depth)\n",
        "        trees.append(tree)\n",
        "\n",
        "        depth = get_tree_depth(tree)\n",
        "        depths.append(depth)\n",
        "\n",
        "        preds = np.array([1 if predict(tree, row) == 1 else -1 for _, row in data.iterrows()])\n",
        "        miss = (preds != y_true).astype(int)\n",
        "        error = np.sum(weights * miss)\n",
        "\n",
        "        if error == 0:\n",
        "            alphas.append(1)\n",
        "            break\n",
        "        if error >= 0.5:\n",
        "            break\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error) / (error + 1e-9))  # +1e-9 supaya tidak pembagian nol\n",
        "        alphas.append(alpha)\n",
        "        weights *= np.exp(-alpha * y_true * preds)\n",
        "        weights /= np.sum(weights)\n",
        "    return trees, alphas, depths\n",
        "\n",
        "def ada_boost_predict(trees, alphas, row):\n",
        "    weighted_votes = sum(alpha * (1 if predict(tree, row) == 1 else -1)\n",
        "                         for tree, alpha in zip(trees, alphas))\n",
        "    return 1 if weighted_votes > 0 else 0\n",
        "\n",
        "# ====================\n",
        "# === Load Dataset ===\n",
        "# ====================\n",
        "df = pd.read_excel(\"dataset_31_credit-g.xlsx\")\n",
        "\n",
        "# Pilih kolom relevan\n",
        "data = df[['class', 'age', 'job', 'credit_amount', 'duration',\n",
        "           'checking_status', 'purpose', 'savings_status', 'personal_status']].copy()\n",
        "\n",
        "# Encoding kolom bertipe objek\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == object:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Pisahkan fitur dan target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']\n",
        "\n",
        "# Gabungkan kembali fitur dan target ke satu dataframe agar fungsi bisa jalan\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# ================================\n",
        "# === Pelatihan & Evaluasi ===\n",
        "# ================================\n",
        "start = time.time()\n",
        "attributes = list(train_data.columns)\n",
        "attributes.remove('class')\n",
        "\n",
        "max_tree_depth = 5  # Set kedalaman maksimum pohon\n",
        "\n",
        "trees, alphas, depths = ada_boost_train(train_data, 'class', attributes,\n",
        "                                        n_estimators=10, max_depth=max_tree_depth)\n",
        "end = time.time()\n",
        "\n",
        "y_pred = [ada_boost_predict(trees, alphas, row) for _, row in test_data.iterrows()]\n",
        "y_true = test_data['class'].values\n",
        "\n",
        "print(f\"\\nAkurasi: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "print(f\"Waktu eksekusi: {end - start:.2f} detik\")\n",
        "\n",
        "# ==============================\n",
        "# === Tampilkan Kedalaman Pohon ===\n",
        "# ==============================\n",
        "print(\"\\nKedalaman masing-masing pohon:\")\n",
        "for i, d in enumerate(depths, start=1):\n",
        "    print(f\"Pohon ke-{i}: kedalaman = {d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMjH3l2gK7yi",
        "outputId": "8f193ebc-c76f-42a5-bf56-dc780afcdbd1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi: 0.7150\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.03      0.07        59\n",
            "           1       0.71      1.00      0.83       141\n",
            "\n",
            "    accuracy                           0.71       200\n",
            "   macro avg       0.86      0.52      0.45       200\n",
            "weighted avg       0.80      0.71      0.61       200\n",
            "\n",
            "Waktu eksekusi: 19.98 detik\n",
            "\n",
            "Kedalaman masing-masing pohon:\n",
            "Pohon ke-1: kedalaman = 5\n",
            "Pohon ke-2: kedalaman = 5\n",
            "Pohon ke-3: kedalaman = 5\n",
            "Pohon ke-4: kedalaman = 5\n"
          ]
        }
      ]
    }
  ]
}